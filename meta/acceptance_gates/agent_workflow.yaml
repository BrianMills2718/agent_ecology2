# Feature: Configurable Agent Workflows
# Flexible per-agent execution workflows with code and LLM steps

feature: agent_workflow
planning_mode: detailed
status: draft  # Pending human review

# === PRD SECTION (What/Why) ===
problem: |
  Current agent architecture has a fixed execution model:
  - Single `decide_action()` → `execute_action()` interface
  - No configurable workflow steps
  - No way for agents to experiment with different execution patterns
  - Agents observed getting stuck (0 writes in 10 ticks) partly due to inflexible execution

  Research (45 sources) shows effective agents need:
  - Observation, planning, AND reflection (Generative Agents: "all 3 required")
  - Self-verification before committing (VOYAGER: -73% without it)
  - Configurable reasoning approaches

  For emergence, agents need to experiment with different workflows to discover
  what works. Fixed architecture prevents this experimentation.

goals:
  - Enable per-agent configurable workflows
  - Support both code and LLM prompt steps
  - Allow agents to self-modify their workflows
  - Maintain minimal kernel (workflow logic is agent-side, not kernel)
  - Keep resource accounting unchanged (kernel tracks LLM calls/actions)
  - Return all errors to agents for self-correction

out_of_scope:
  - "Genesis artifacts (design assumes no genesis for simplicity)"
  - "Cross-agent real-time messaging (artifact-mediated only)"
  - "Provider-specific LLM features (pragmatic: Gemini for now)"
  - "Multi-agent orchestration patterns (agents build these themselves)"

# === PHASED IMPLEMENTATION ===
# Start simple, add power incrementally. Don't lock out future options.

phases:
  phase_1_mvp:
    name: "Simple Workflow Execution"
    goal: "Agents can define and execute multi-step workflows"
    scope:
      - "Workflow = ordered list of steps"
      - "Step types: code, llm"
      - "LLM steps reference prompt artifacts (agent writes these)"
      - "Code steps execute Python expressions"
      - "Sequential execution only"
      - "Errors returned to agent"
      - "Workflow self-modification (applies next iteration)"
    not_included:
      - "Context injection DSL"
      - "Output schemas"
      - "Parallel steps"
      - "Template rendering"
    config_example: |
      workflow:
        - step: prepare
          type: code
          code: "context.memories = self.memory.search(context.goal)"

        - step: think
          type: llm
          prompt_artifact: "prompts/think.md"  # Agent writes this artifact

        - step: act
          type: llm
          prompt_artifact: "prompts/act.md"

      model: "gemini/gemini-2.0-flash"
      error_handling:
        default_on_failure: retry
        max_retries: 3
    notes: |
      Agent is responsible for constructing prompts with context.
      No magic injection - agent explicitly puts data in prompt artifact.
      Simple, observable, debuggable.

  phase_2_context_injection:
    name: "Declarative Context Injection"
    goal: "Prompts are templates with automatic context filling"
    prerequisites:
      - "Phase 1 working and stable"
      - "Observed need for context injection (agents struggling to construct prompts)"
    scope:
      - "Prompt templates with placeholders (e.g., {{memories}})"
      - "inject: block defines what fills placeholders"
      - "Predefined injection sources (context.*, self.*, world.*)"
      - "Template syntax: Mustache or similar (simple, safe)"
    not_included:
      - "Arbitrary code in injection sources"
      - "Output schemas"
      - "Agent-defined injection sources"
    config_example: |
      workflow:
        - step: think
          type: llm
          prompt_template: "prompts/think.md"
          inject:
            agent_id: self.id
            memories: context.memories
            world_state: world.get_summary()
            recent_actions: self.action_history[-5:]
    open_questions:
      - "Template language choice (Mustache vs Jinja2 vs custom)"
      - "Exact injection source syntax and available sources"
      - "Who renders templates - kernel or agent-side library?"

  phase_3_output_schemas:
    name: "Structured Output Schemas"
    goal: "LLM outputs are validated against schemas"
    prerequisites:
      - "Phase 2 working"
      - "Observed need for structured outputs (agents getting malformed responses)"
    scope:
      - "output_schema: references a Pydantic model"
      - "LLM response validated against schema"
      - "Validation errors returned to agent"
      - "Structured output available to subsequent steps"
    not_included:
      - "Agent-defined schemas (schemas are predefined)"
    config_example: |
      workflow:
        - step: think
          type: llm
          prompt_template: "prompts/think.md"
          inject:
            memories: context.memories
          output_schema: ThinkingOutput  # Predefined Pydantic model

        - step: act
          type: llm
          prompt_template: "prompts/act.md"
          inject:
            thinking: steps.think.output  # Structured output from previous step
          output_schema: ActionOutput
    open_questions:
      - "Where do schema definitions live?"
      - "Can agents select from a library of schemas, or fixed per step type?"
      - "Does schema enforcement help or hurt emergence?"

  phase_4_full_meta_config:
    name: "Full Meta-Configurability"
    goal: "Agents can define their own templates, injections, and schemas"
    prerequisites:
      - "Phase 3 working"
      - "Evidence that meta-config helps (not just complexity)"
    scope:
      - "Agent-defined prompt templates (as artifacts)"
      - "Agent-defined output schemas (as artifacts)"
      - "Dynamic injection sources"
      - "Conditional injection (only inject if value exists)"
      - "Parallel step execution"
    risks:
      - "Complexity may degrade agent performance"
      - "Security: arbitrary injection sources could leak data"
      - "Debugging: more layers = harder to trace failures"
    notes: |
      This phase is speculative. May not be needed if earlier phases suffice.
      Inspiration: static_pipeline's meta-configurable prompt system.
      Key insight: separate Template (what prompt looks like) from
      Injections (what data fills it) from Schema (what output looks like).

# === DESIGN SECTION (How) - Phase 1 Focus ===
design:
  approach: |
    Phase 1: Each agent defines a workflow in its config - a sequence of steps
    that execute each iteration. Steps can be executable code or LLM calls.

    The kernel provides:
    - Artifact storage (read/write/invoke)
    - Agent loop execution
    - Resource tracking (LLM calls, actions)
    - Basic config validation (schema only)
    - Error return to agents

    Everything else is agent-side:
    - Workflow step execution
    - Prompt construction (agent writes prompt artifacts)
    - Internal reasoning/verification
    - Learning, planning, reflection

  architecture_phase_1: |
    Agent Config Structure (Phase 1 - Simple):
    ```yaml
    workflow:
      - step: prepare
        type: code
        code: "context.memories = self.memory.search(context.goal)"

      - step: periodic_synthesis
        type: code
        run_if: "self.iteration_count % 100 == 0"
        code: "self.synthesize_experience()"

      - step: think
        type: llm
        prompt_artifact: "prompts/think.md"
        on_failure: retry

      - step: act
        type: llm
        prompt_artifact: "prompts/act.md"

    model: "gemini/gemini-2.0-flash"
    actions_per_iteration: 1
    memory_mode: automatic

    error_handling:
      default_on_failure: retry
      max_retries: 3
    ```

    Agent writes prompt artifacts like `prompts/think.md`:
    ```markdown
    You are agent {self.id} in an ecology simulation.

    ## Your Goal
    {self.goal}

    ## Recent Memories
    {context.memories}

    ## World State
    {world.summary}

    Based on this context, what should you think about?
    ```

    Note: In Phase 1, agent must construct this prompt themselves.
    No automatic injection. Agent reads artifacts, builds prompt, writes it.

  key_decisions:
    - "Workflow = list of steps (code and/or LLM prompts)"
    - "Steps execute sequentially within an iteration"
    - "Kernel validates config schema only, not workflow semantics"
    - "Validation errors returned to agent (not silent, not crash)"
    - "Runtime errors returned to agent for self-correction"
    - "Resource accounting unchanged - kernel tracks LLM calls/actions"
    - "Internal workflow steps NOT logged by kernel (agent's choice)"
    - "Workflow changes apply next iteration, not mid-iteration"
    - "Error handling is agent-configurable with system defaults"
    - "Phase 1 has no injection DSL - agent constructs prompts manually"
    - "Future phases add injection/schemas incrementally based on observed need"

  risks:
    - "Workflow complexity could degrade agent performance (mitigated: agents can use simple workflows)"
    - "Infinite loops in code steps (mitigated: iteration timeout, resource limits)"
    - "Invalid workflows (mitigated: schema validation, fail-fast runtime, errors returned)"
    - "Premature complexity (mitigated: phased approach, add features only when needed)"

# === RESEARCH NOTES ===
# Captured from analysis of static_pipeline and agent framework comparison
research_notes:
  static_pipeline_patterns: |
    Analyzed https://github.com/BrianMills2718/static_pipeline for relevant patterns.

    Key patterns identified:
    1. BaseStage pattern: Abstract class with execute() + run() wrapper for cross-cutting
    2. StageResult: Uniform return type (success, message, duration, data)
    3. Prompt templates with placeholders: {feedback_data}, {kb_summary}, etc.
    4. load_template() fills placeholders at runtime
    5. Pydantic schemas for all LLM outputs
    6. Anti-gaming validators in schemas (field_validator decorators)
    7. Audit pattern: flag issues but don't block, collect in report

    What transfers:
    - Step abstraction pattern
    - Template + injection separation concept
    - Structured output with schemas
    - Error collection pattern

    What doesn't transfer:
    - Centralized orchestration (we want agent-side)
    - Fixed stage sequence (we want agent-defined)
    - Domain-specific stages (prompt engineering specific)

  framework_comparison: |
    Compared LangGraph, AutoGen, CrewAI to our needs.

    These frameworks solve: "How do I (developer) orchestrate multiple agents?"
    We need to solve: "How do agents learn to orchestrate themselves?"

    Key mismatch:
    - They assume developer designs workflow → we want agent designs workflow
    - They assume multi-agent coordination → we need single-agent intelligence first
    - They assume static workflows → we need self-modifying workflows

    Decision: Don't adopt these frameworks. Build simple workflow execution.
    The interesting part is what agents PUT in workflows, not the execution.

  meta_config_tradeoffs: |
    Analysis of meta-configurable prompts (Phase 4 considerations):

    Potential benefits:
    - Agents can modify templates without code changes
    - Separation of template/injection/schema is clean
    - Enables experimentation with prompt structure

    Potential downsides:
    - Complexity tax on agents (must learn DSL)
    - Injection source security (what can agents access?)
    - Debugging difficulty (more layers)
    - Schema enforcement may limit emergence

    Open questions:
    - Do agents actually need to modify prompts?
    - Is the value in workflow modification, not prompt modification?
    - How much structure helps vs hurts emergence?

    Decision: Defer to Phase 4. Start simple. Add based on observed need.

# === ACCEPTANCE CRITERIA (Phase 1) ===
acceptance_criteria:
  - id: AC-1
    scenario: "Agent runs simple workflow"
    phase: 1
    category: happy_path
    given:
      - "Agent configured with workflow: [think, act]"
      - "Both steps reference valid prompt artifacts"
    when: "Agent iteration executes"
    then:
      - "Think step runs, produces reasoning"
      - "Act step runs, produces action"
      - "Action executed via narrow waist"
    locked: false

  - id: AC-2
    scenario: "Workflow with code step"
    phase: 1
    category: happy_path
    given:
      - "Agent configured with code step before LLM step"
      - "Code step sets context.memories"
    when: "Agent iteration executes"
    then:
      - "Code step runs first"
      - "LLM step can access context set by code step"
    locked: false

  - id: AC-3
    scenario: "Conditional code step"
    phase: 1
    category: happy_path
    given:
      - "Agent configured with step having run_if condition"
      - "Condition evaluates to false"
    when: "Agent iteration executes"
    then:
      - "Step is skipped"
      - "Subsequent steps still run"
    locked: false

  - id: AC-4
    scenario: "Workflow validation error returned to agent"
    phase: 1
    category: error_case
    given:
      - "Agent submits workflow with invalid step type"
    when: "Kernel validates config"
    then:
      - "Validation fails"
      - "Error returned to agent with details"
      - "Agent can read error and self-correct"
    locked: false

  - id: AC-5
    scenario: "Runtime error returned to agent"
    phase: 1
    category: error_case
    given:
      - "Agent workflow references non-existent prompt artifact"
    when: "Step executes"
    then:
      - "Step fails"
      - "Error returned to agent with details"
      - "Agent can modify workflow or retry"
    locked: false

  - id: AC-6
    scenario: "Agent self-modifies workflow"
    phase: 1
    category: happy_path
    given:
      - "Agent running with workflow A"
      - "Agent writes new workflow B to its config"
    when: "Next iteration starts"
    then:
      - "New workflow B is loaded"
      - "Agent runs with workflow B"
    locked: false

  - id: AC-7
    scenario: "Error handling respects agent config"
    phase: 1
    category: happy_path
    given:
      - "Agent configured with on_failure: skip for verify step"
      - "Verify step fails"
    when: "Workflow continues"
    then:
      - "Verify step skipped"
      - "Subsequent steps still run"
      - "Error info available to agent"
    locked: false

  - id: AC-11
    scenario: "Agent defines empty workflow"
    phase: 1
    category: edge_case
    given:
      - "Agent configured with workflow containing zero steps"
    when: "Agent iteration executes"
    then:
      - "Iteration completes without error"
      - "No actions taken"
      - "Agent can still modify workflow for next iteration"
    locked: false

  - id: AC-12
    scenario: "Code step throws exception"
    phase: 1
    category: edge_case
    given:
      - "Agent workflow has code step that raises Python exception"
      - "Step has on_failure: continue configured"
    when: "Code step executes"
    then:
      - "Exception caught and logged"
      - "Error returned to agent"
      - "Subsequent steps still run"
      - "Agent can inspect error details"
    locked: false

  # Phase 2 acceptance criteria (for future)
  - id: AC-8
    scenario: "Context injection fills template placeholders"
    phase: 2
    category: happy_path
    given:
      - "Agent configured with inject: block"
      - "Prompt template has {{memories}} placeholder"
    when: "LLM step executes"
    then:
      - "Template rendered with injected context"
      - "LLM receives filled prompt"
    locked: false

  # Phase 3 acceptance criteria (for future)
  - id: AC-9
    scenario: "Output schema validates LLM response"
    phase: 3
    category: happy_path
    given:
      - "Step configured with output_schema: ThinkingOutput"
      - "LLM returns valid JSON matching schema"
    when: "Step completes"
    then:
      - "Output parsed and validated"
      - "Structured output available to next step"
    locked: false

  - id: AC-10
    scenario: "Schema validation failure returned to agent"
    phase: 3
    category: error_case
    given:
      - "Step configured with output_schema"
      - "LLM returns invalid JSON"
    when: "Validation runs"
    then:
      - "Validation fails"
      - "Error returned to agent with details"
      - "Agent can retry or modify prompt"
    locked: false

# === IMPLEMENTATION SECTION ===
adrs:
  - ADR-0013  # Configurable Agent Workflows

code:
  phase_1:
    - src/agents/workflow.py       # New: workflow execution engine
    - src/agents/agent.py          # Modified: use workflow instead of decide/execute
    - src/simulation/agent_loop.py # Modified: call workflow.run()
  phase_2:
    - src/agents/template.py       # New: template rendering with injection
  phase_3:
    - src/agents/schemas.py        # New: output schema definitions and validation

tests:
  phase_1:
    - tests/unit/test_workflow.py
    - tests/integration/test_agent_workflow.py
  phase_2:
    - tests/unit/test_template_injection.py
  phase_3:
    - tests/unit/test_output_schemas.py

docs:
  - docs/architecture/current/agents.md          # Update for workflow model
  - docs/architecture/current/execution_model.md # Update agent interface
