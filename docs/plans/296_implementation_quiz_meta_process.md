# Plan 296: Implementation Understanding Quiz Meta-Process

**Status:** âœ… Complete

**Verified:** 2026-02-05T15:19:27Z
**Verification Evidence:**
```yaml
completed_by: scripts/complete_plan.py
timestamp: 2026-02-05T15:19:27Z
tests:
  unit: skipped (--status-only, CI-validated)
  e2e_smoke: skipped (--status-only, CI-validated)
  e2e_real: skipped (--status-only, CI-validated)
  doc_coupling: skipped (--status-only, CI-validated)
commit: 7f4ec53
```
**Priority:** High (process improvement)
**Blocked By:** None
**Blocks:** None

---

## Problem

After planning and implementation, the user is often surprised by:
- What actually exists in code files
- Tradeoffs that were made implicitly
- Divergence between mental model and implementation

This creates technical debt in understanding - the codebase drifts from the user's intentions without explicit acknowledgment.

### Root Causes

1. **Plans describe WHAT, not HOW**: Plans focus on outcomes but implementation requires many micro-decisions
2. **Silent tradeoffs**: Implementer makes reasonable choices but doesn't surface them
3. **Review gap**: Code review catches bugs but not intention misalignment
4. **Accumulating confusion**: Each small divergence compounds

### Current Gap

No systematic way to verify:
- "Does the user understand what this file actually does?"
- "Does the user agree with the tradeoffs made?"
- "Does the implementation match the user's mental model?"

---

## Proposed Solution: Implementation Understanding Quiz

After significant code changes, generate a quiz that:
1. Surfaces key design decisions in each file
2. Asks about tradeoffs and alternatives
3. Verifies understanding of what was built
4. Catches misalignment before it compounds

### When to Quiz

**Option A: Post-Implementation (before merge)**
- After code is written, before PR merge
- Quiz: "Here's what was built - do you understand and agree?"
- Catches surprises before they're committed

**Option B: Pre-Implementation (after planning)**
- After plan is written, before coding
- Quiz: "Here's what the plan will create - is this what you expect?"
- Catches misalignment before implementation effort

**Option C: Both**
- Pre-implementation: Quiz on plan implications
- Post-implementation: Quiz on actual code
- Most thorough but higher overhead

**Recommendation: Start with Post-Implementation**
- Easier to quiz on concrete code than abstract plans
- Catches the actual surprises
- Can add pre-implementation later if valuable

---

## Quiz Format

### Per-File Quiz

For each significantly changed file:

```markdown
## Quiz: src/world/ledger.py

### Q1: Core Responsibility
This file's primary responsibility is:
- [ ] A) Managing artifact storage
- [ ] B) Tracking principal resource quotas and scrip balances
- [ ] C) Executing agent actions
- [ ] D) Handling LLM API calls

### Q2: Key Tradeoff
The ledger stores quotas as a flat dict rather than typed objects. This means:
- [ ] A) Easier to add new resource types without schema changes
- [ ] B) No compile-time checking of quota names
- [ ] C) Both A and B
- [ ] D) Neither

### Q3: Design Decision
When a principal tries to spend more than their balance:
- [ ] A) The transaction silently fails
- [ ] B) An exception is raised (fail loud)
- [ ] C) The balance goes negative
- [ ] D) A warning is logged and it succeeds

### Q4: Agree/Disagree
"The ledger should track quota history for auditing."
- [ ] Agree - this should be added
- [ ] Disagree - current design is correct
- [ ] Defer - not important now

### Q5: Surprise Check
Was anything in this file surprising or different from what you expected?
- [ ] No, matches my expectations
- [ ] Yes (please explain): ___________
```

### Quiz Generation

The quiz can be generated by:
1. **AI analysis** - Claude reads the file and generates questions about key decisions
2. **Template-based** - Standard questions for each file type (schema, executor, loader, etc.)
3. **Diff-based** - Questions focused on what changed, not entire file

---

## Implementation

### Phase 1: Manual Quiz Generation

1. After PR is ready, before merge
2. Claude generates quiz for changed files
3. User answers in conversation
4. Misalignments discussed and resolved
5. PR merged (or revised)

**Command**: `/quiz` or automatic when `make pr` is run

### Phase 2: Quiz Templates

Create templates for common file types:

```yaml
# meta-process/quiz_templates/schema.yaml
questions:
  - type: multiple_choice
    template: "What happens when validation fails for {model_name}?"
    options:
      - "Exception raised"
      - "Default value used"
      - "Warning logged"
      - "Depends on field"

  - type: tradeoff
    template: "The schema uses {approach}. Alternative was {alternative}. Trade-off is:"

  - type: agree_disagree
    template: "This schema should also validate {potential_addition}"
```

### Phase 3: Quiz Records

Store quiz results for future reference:

```
docs/quizzes/
  295_resource_gating/
    ledger.md      # Quiz answers and discussion
    kernel_interface.md
```

This creates a record of:
- What decisions were made
- Why they were made
- User's explicit agreement

### Phase 4: Pre-Implementation Quiz

After plan is written, quiz on implications:

```markdown
## Plan Implications Quiz: Plan 295

### Q1: This plan will remove `has_loop`. What breaks?
- [ ] Agent loading from YAML
- [ ] Simulation runner scheduling
- [ ] Both
- [ ] Neither (it's just metadata)

### Q2: Resource gating at kernel means:
- [ ] A) More latency (checks on every operation)
- [ ] B) More code in kernel interface
- [ ] C) Artifacts can't bypass limits
- [ ] D) All of the above

### Q3: After this plan, how does an artifact start running?
[Free response - verifies user understands the new model]
```

---

## Integration with Meta-Process

### Hook: Pre-Merge Quiz

```yaml
# meta-process.yaml
hooks:
  pre_merge:
    - name: implementation_quiz
      when: "files_changed > 3 OR lines_changed > 100"
      action: "generate_quiz"
      blocking: true  # Can't merge until quiz passed
```

### Skill: /quiz

```
/quiz [file_or_plan]   # Generate quiz for specific file or current plan
/quiz --diff           # Quiz only on changed files
/quiz --pre            # Pre-implementation quiz on plan
```

---

## Open Questions

1. **Quiz fatigue**: How do we prevent this from becoming annoying overhead?
   - Only for significant changes?
   - Skippable with `--no-quiz` flag?
   - Adapt frequency based on surprise rate?

2. **Quiz quality**: How do we ensure questions are meaningful?
   - Templates help consistency
   - AI-generated catches nuance
   - Iterate based on feedback

3. **Wrong answers**: What happens when user gets questions "wrong"?
   - Not a test - it's alignment check
   - "Wrong" answer = either user misunderstands or implementation is wrong
   - Either way, discussion needed

4. **Async quiz**: Should quiz be answerable later?
   - Maybe user wants to merge now, quiz later
   - Risk: never gets done
   - Compromise: quiz required within N days

---

## Success Criteria

- [ ] Users report fewer surprises in implemented code
- [ ] Tradeoffs are explicitly documented via quiz records
- [ ] Misalignments caught before merge
- [ ] Mental model drift reduced

---

## Notes

The goal isn't "testing the user" - it's surfacing implicit decisions and ensuring alignment. The quiz is a forcing function for:
1. Implementer to articulate decisions
2. User to engage with actual implementation
3. Both to agree on tradeoffs

This addresses the gap between "what I asked for" and "what got built" - which is one of the hardest problems in collaborative development with AI.
