# WS-6 Infrastructure & Observability Gaps
#
# Comparison of:
#   Current: docs/architecture/current/supporting_systems.md
#   Target: docs/architecture/target/07_infrastructure.md
#
# Created: 2026-01-12
# Methodology: GAP_IDENTIFICATION_METHODOLOGY.md

metadata:
  workstream: WS-6
  component: infrastructure
  created: 2026-01-12
  sources:
    current: docs/architecture/current/supporting_systems.md
    target: docs/architecture/target/07_infrastructure.md

gaps:
  # ============================================================================
  # DOCKER RESOURCE ISOLATION
  # ============================================================================

  - id: GAP-INFRA-001
    component: infrastructure
    dimension: capabilities
    title: "Docker containerization not implemented"
    current_state: |
      No Docker containerization. Simulation runs directly on host system
      without resource isolation. Agents share host resources without
      hard limits.
    target_state: |
      Docker container with hard resource limits:
      - `--memory=4g` for RAM cap
      - `--cpus=2` for CPU limit
      - `--storage-opt` for disk limits
      Host stays responsive even if agents misbehave.
    delta: |
      Need to:
      - Create Dockerfile for agent ecology
      - Configure container resource limits
      - Test that limits are actually enforced
      - Document container configuration
    dependencies: []
    complexity: M
    risk: medium
    files_affected:
      - Dockerfile (new)
      - docker-compose.yml (new)
      - README.md (updates for Docker instructions)
    acceptance_criteria:
      - Dockerfile exists and builds successfully
      - Container runs with memory and CPU limits
      - Simulation continues when agent attempts to exceed limits
      - Host system remains responsive during heavy simulation

  - id: GAP-INFRA-002
    component: infrastructure
    dimension: configuration
    title: "No container architecture options (single vs. separate)"
    current_state: |
      No container architecture. Qdrant and agent ecology run as
      separate processes on host without isolation.
    target_state: |
      Two architecture options:
      1. Single container - simpler, all resources shared
      2. Separate containers - better isolation, agents can't starve Qdrant

      Separate containers use different resource allocations:
      - Agent ecology: 4GB, 2 CPU
      - Qdrant: 2GB, 1 CPU
    delta: |
      Need to:
      - Implement single-container Dockerfile option
      - Implement docker-compose for multi-container setup
      - Document trade-offs between options
    dependencies:
      - GAP-INFRA-001
    complexity: M
    risk: low
    files_affected:
      - Dockerfile (single container option)
      - docker-compose.yml (multi-container option)
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - Both single and multi-container configs exist
      - Documentation explains when to use each
      - Multi-container setup prevents Qdrant starvation

  # ============================================================================
  # RESOURCE MAPPING & CALIBRATION
  # ============================================================================

  - id: GAP-INFRA-003
    component: infrastructure
    dimension: behaviors
    title: "No token bucket rate calibration to container CPU"
    current_state: |
      Token bucket rates are configured statically in config.yaml
      without relationship to actual container CPU capacity.
    target_state: |
      Token bucket rate calibrated to container CPU:
      - Container has N CPUs
      - Token rate set so max concurrent agents don't exceed capacity
      - Calibration algorithm documents how to determine optimal rate

      Suggested starting rates:
      - Laptop (4 cores): 5-10 tokens/sec
      - Desktop (8 cores): 10-20 tokens/sec
      - Server (16+ cores): 20-50 tokens/sec
    delta: |
      Need to:
      - Document calibration process
      - Add calibration script/tooling
      - Provide hardware-specific recommended settings
      - Link container limits to config defaults
    dependencies:
      - GAP-INFRA-001
    complexity: M
    risk: low
    files_affected:
      - config/config.yaml
      - scripts/calibrate_resources.py (new)
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - Calibration process documented
      - Sample config for different hardware classes
      - CPU usage stabilizes at 70-80% during stress test

  - id: GAP-INFRA-004
    component: infrastructure
    dimension: behaviors
    title: "No stress testing tooling for calibration"
    current_state: |
      No built-in stress testing capability. Cannot easily run
      calibration tests with multiple agents at full speed.
    target_state: |
      Stress test capability:
      1. Start N agents in continuous mode
      2. Monitor container stats (`docker stats`)
      3. Observe CPU usage (target 70-80% sustained)
      4. Observe memory usage (target <90% of limit)
      5. Watch for throttling indicators

      Calibration algorithm:
      1. Start with rate = 10 tokens/sec per agent
      2. Run 5 agents at full continuous loop for 5 minutes
      3. If CPU > 85%: reduce rate by 20%
      4. If CPU < 50%: increase rate by 25%
      5. Repeat until CPU stabilizes at 70-80%
    delta: |
      Need to:
      - Create stress test script
      - Implement calibration algorithm
      - Add monitoring integration for docker stats
    dependencies:
      - GAP-INFRA-001
      - GAP-INFRA-003
    complexity: M
    risk: low
    files_affected:
      - scripts/stress_test.py (new)
      - scripts/calibrate_resources.py (new)
    acceptance_criteria:
      - Stress test can run N agents continuously
      - Script reports CPU/memory usage
      - Calibration provides recommended token rate

  - id: GAP-INFRA-005
    component: infrastructure
    dimension: configuration
    title: "No memory-to-agent mapping documentation"
    current_state: |
      No documentation of how memory limits translate to
      maximum concurrent agents.
    target_state: |
      Memory mapping documented:
      - Container: 4GB
      - Per-agent memory: ~200-500MB
      - Max concurrent thinking agents: ~8-20

      This informs agent count configuration.
    delta: |
      Need to:
      - Profile actual agent memory usage
      - Document memory per agent
      - Create config guidance based on container memory
    dependencies: []
    complexity: S
    risk: low
    files_affected:
      - docs/architecture/current/supporting_systems.md
      - config/schema.yaml (documentation)
    acceptance_criteria:
      - Memory usage per agent profiled
      - Documentation provides max agent guidelines
      - Config includes memory-based agent limits

  # ============================================================================
  # MULTI-CONTAINER SCALING
  # ============================================================================

  - id: GAP-INFRA-006
    component: infrastructure
    dimension: capabilities
    title: "No PostgreSQL support for distributed ledger"
    current_state: |
      Ledger uses in-memory dict or SQLite. Not suitable for
      multi-container deployment due to file locking issues.
    target_state: |
      Distributed ledger with PostgreSQL:
      - All ledger operations through shared database
      - Transactions ensure atomicity across containers
      - Connection pooling per agent node (min_size=5, max_size=20)

      Example:
      ```python
      class DistributedLedger:
          def __init__(self, pg_url: str):
              self.pool = asyncpg.create_pool(pg_url, min_size=5, max_size=20)
      ```
    delta: |
      Need to:
      - Add asyncpg dependency
      - Implement DistributedLedger class
      - Migration from SQLite to PostgreSQL
      - Connection pooling configuration
    dependencies: []
    complexity: L
    risk: medium
    files_affected:
      - src/world/ledger.py
      - requirements.txt
      - config/config.yaml
      - docker-compose.yml
    acceptance_criteria:
      - PostgreSQL backend works as alternative to SQLite
      - Transactions are atomic across connections
      - Connection pooling configured appropriately
      - Migration path from SQLite documented

  - id: GAP-INFRA-007
    component: infrastructure
    dimension: capabilities
    title: "No Redis event bus for cross-container events"
    current_state: |
      No event bus. Events are logged to JSONL file only.
      Cannot notify agents in other containers.
    target_state: |
      Distributed event bus with Redis:
      - Pub/sub for cross-container events
      - Agents in any container can wake agents in any other container
      - Fire-and-forget (no event queuing, agents poll on restart)

      Example:
      ```python
      class DistributedEventBus:
          async def publish(self, event: Event) -> None:
              await self.redis.publish(f"events:{event.type}", event.json())
          async def subscribe(self, agent_id: str, event_type: str) -> None:
              await self.pubsub.subscribe(f"events:{event_type}")
      ```
    delta: |
      Need to:
      - Add aioredis dependency
      - Implement DistributedEventBus class
      - Integrate with existing EventLogger
      - Configure pub/sub channels
    dependencies: []
    complexity: L
    risk: medium
    files_affected:
      - src/world/logger.py
      - src/world/event_bus.py (new)
      - requirements.txt
      - config/config.yaml
      - docker-compose.yml
    acceptance_criteria:
      - Events published to Redis channels
      - Agents can subscribe to event types
      - Cross-container event delivery works
      - Graceful degradation if Redis unavailable

  - id: GAP-INFRA-008
    component: infrastructure
    dimension: capabilities
    title: "No static agent-to-container assignment"
    current_state: |
      All agents run in single process. No concept of
      distributing agents across containers.
    target_state: |
      Static agent assignment:
      - Agents assigned to containers at startup
      - Environment variable AGENT_RANGE specifies which agents
      - Example: AGENT_RANGE="1-50" for first container

      Benefits:
      - Simpler than dynamic migration
      - Predictable - know where each agent runs
      - Sufficient for initial scaling
    delta: |
      Need to:
      - Add AGENT_RANGE environment variable support
      - Modify agent loader to filter by range
      - Update docker-compose with per-node config
    dependencies:
      - GAP-INFRA-006
      - GAP-INFRA-007
    complexity: M
    risk: low
    files_affected:
      - src/agents/loader.py
      - run.py
      - docker-compose.yml
    acceptance_criteria:
      - AGENT_RANGE env var parsed correctly
      - Each container only loads assigned agents
      - All agents covered without overlap

  - id: GAP-INFRA-009
    component: infrastructure
    dimension: configuration
    title: "No docker-compose for multi-container scaling"
    current_state: |
      No docker-compose configuration. Multi-container
      deployment not supported.
    target_state: |
      Complete docker-compose with:
      - PostgreSQL service (2GB, 1 CPU)
      - Redis service (1GB, 1 CPU)
      - Qdrant service (4GB, 2 CPU)
      - Agent nodes (4GB, 2 CPU each, replicas: N)
      - Shared volumes for persistence
      - Environment variables for service URLs
    delta: |
      Need to:
      - Create docker-compose.yml with all services
      - Configure resource limits per service
      - Set up shared networking
      - Configure volumes for persistence
    dependencies:
      - GAP-INFRA-001
      - GAP-INFRA-006
      - GAP-INFRA-007
    complexity: M
    risk: low
    files_affected:
      - docker-compose.yml (new or expanded)
    acceptance_criteria:
      - docker-compose up starts all services
      - Services can communicate over network
      - Resource limits enforced per service
      - Data persists across restarts

  - id: GAP-INFRA-010
    component: infrastructure
    dimension: interfaces
    title: "No cross-container interaction transparency"
    current_state: |
      All agent interactions are local. No network
      latency considerations.
    target_state: |
      Cross-container operations transparent to agents:
      - Transfer scrip: PostgreSQL transaction (transparent)
      - Read artifact: PostgreSQL query (transparent)
      - Wake sleeping agent: Redis pub/sub (transparent)
      - Access memory: Qdrant query (transparent)

      Latency: ~1-5ms additional for cross-container ops.
      Acceptable for LLM-driven agents (thinking takes seconds).
    delta: |
      Need to:
      - Ensure all operations work through network services
      - Abstract storage backends behind interfaces
      - Handle network failures gracefully
    dependencies:
      - GAP-INFRA-006
      - GAP-INFRA-007
    complexity: L
    risk: medium
    files_affected:
      - src/world/ledger.py
      - src/world/artifacts.py
      - src/agents/memory.py
    acceptance_criteria:
      - Same API works for local and distributed mode
      - Network latency doesn't break timeouts
      - Graceful handling of network errors

  # ============================================================================
  # LOCAL LLM SUPPORT
  # ============================================================================

  - id: GAP-INFRA-011
    component: infrastructure
    dimension: capabilities
    title: "No local LLM support"
    current_state: |
      Only cloud LLM APIs supported (Anthropic, OpenAI, etc.).
      No local model option.
    target_state: |
      Two local LLM patterns:
      1. CPU-only (llama.cpp): Run in worker pool, resource.getrusage()
         captures compute automatically
      2. GPU-based (vLLM, TGI): Separate model server, GPU metrics via
         server API

      Resource tracking differs by setup.
    delta: |
      Need to:
      - Add llama.cpp integration option
      - Add vLLM/TGI client support
      - Implement appropriate resource tracking per pattern
      - Document configuration for each option
    dependencies: []
    complexity: XL
    risk: high
    files_affected:
      - src/agents/llm.py
      - config/config.yaml
      - requirements.txt
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - llama.cpp backend works for CPU-only
      - GPU-based backend works with external server
      - Resource usage tracked appropriately per backend
      - Documentation covers setup for each option

  # ============================================================================
  # GIT-BACKED ARTIFACT STORE
  # ============================================================================

  - id: GAP-INFRA-012
    component: infrastructure
    dimension: capabilities
    title: "No Git-backed artifact store"
    current_state: |
      Artifacts stored in memory dict or SQLite.
      No history tracking, no audit trail.
    target_state: |
      Git-backed artifact store:
      - Current commit (HEAD) is physical reality agents see
      - write_artifact() creates new commit
      - Disk quotas still apply to working tree

      Git history is observer-only (admin, not agents):
      - Immutable audit trail
      - Safe rollback without agent "time travel"
      - Prevents "history as free storage" exploit
    delta: |
      Need to:
      - Implement GitArtifactStore class
      - Each write creates a commit
      - Expose only HEAD to agents
      - Admin-only access to history
    dependencies: []
    complexity: L
    risk: medium
    files_affected:
      - src/world/artifacts.py
      - src/world/git_store.py (new)
      - requirements.txt (gitpython or similar)
    acceptance_criteria:
      - Artifacts persisted to Git repository
      - Each write creates commit with metadata
      - Agents cannot read history (only current state)
      - Admin can browse/rollback history

  - id: GAP-INFRA-013
    component: infrastructure
    dimension: constraints
    title: "No protection against history-as-free-storage exploit"
    current_state: |
      No artifact history, so exploit not applicable.
      However, no protection mechanism in place.
    target_state: |
      Exploit prevention:
      1. Agent commits 1GB of data
      2. Agent overwrites with new data
      3. Agent tries to retrieve old data from history
      4. BLOCKED - agents can only access current commit

      Future consideration: Limited history access as premium
      feature (expensive retrieval cost scaling with age).
    delta: |
      Need to:
      - Ensure artifact read API only accesses HEAD
      - No API for history access by agents
      - Document exploit and prevention
    dependencies:
      - GAP-INFRA-012
    complexity: S
    risk: low
    files_affected:
      - src/world/artifacts.py
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - No API endpoint exposes Git history to agents
      - Artifact reads always return current state
      - Security model documented

  # ============================================================================
  # MONITORING & RESTART
  # ============================================================================

  - id: GAP-INFRA-014
    component: infrastructure
    dimension: capabilities
    title: "No container monitoring integration"
    current_state: |
      Dashboard shows simulation state (agents, artifacts, events).
      No container-level metrics (CPU, memory, network).
    target_state: |
      Container monitoring includes:
      - Container stats (CPU, memory, network)
      - Agent metrics (actions/sec, blocked status)
      - LLM costs ($)

      Integrate with `docker stats` or similar.
    delta: |
      Need to:
      - Add container stats collection
      - Expose via dashboard API
      - Display in dashboard UI
    dependencies:
      - GAP-INFRA-001
    complexity: M
    risk: low
    files_affected:
      - src/dashboard/server.py
      - src/dashboard/stats_collector.py (new)
      - src/dashboard/static/ (UI updates)
    acceptance_criteria:
      - Dashboard shows CPU/memory usage
      - Updates in real-time
      - Historical charts available

  - id: GAP-INFRA-015
    component: infrastructure
    dimension: configuration
    title: "No automatic container restart policy"
    current_state: |
      No Docker, so no restart policy. Process crash
      requires manual restart.
    target_state: |
      Container restart policy:
      ```bash
      docker run --restart=unless-stopped agent-ecology
      ```

      Automatic recovery from crashes.
    delta: |
      Need to:
      - Configure restart policy in Dockerfile/docker-compose
      - Ensure clean state restoration after restart
      - Log restart events
    dependencies:
      - GAP-INFRA-001
    complexity: S
    risk: low
    files_affected:
      - Dockerfile
      - docker-compose.yml
    acceptance_criteria:
      - Container restarts automatically on crash
      - Simulation resumes from checkpoint
      - Restart events logged

  # ============================================================================
  # WINDOWS CONSIDERATIONS
  # ============================================================================

  - id: GAP-INFRA-016
    component: infrastructure
    dimension: configuration
    title: "No Windows/Docker Desktop documentation"
    current_state: |
      No documentation for running on Windows.
      Implicitly assumes Linux.
    target_state: |
      Windows documentation:
      - Docker Desktop (WSL2 or Hyper-V)
      - Slight overhead vs native Linux (acceptable)
      - How to check container resources: `docker stats agent-ecology`
    delta: |
      Need to:
      - Test on Windows with Docker Desktop
      - Document any Windows-specific configuration
      - Note performance differences
    dependencies:
      - GAP-INFRA-001
    complexity: S
    risk: low
    files_affected:
      - README.md
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - Windows setup documented
      - Any Windows-specific issues noted
      - Performance expectations set

  # ============================================================================
  # KUBERNETES (FUTURE)
  # ============================================================================

  - id: GAP-INFRA-017
    component: infrastructure
    dimension: capabilities
    title: "No Kubernetes support (future)"
    current_state: |
      No Kubernetes support. Single-host deployment only.
    target_state: |
      Kubernetes provides (future, beyond MVP):
      - Automatic container restart on failure
      - Horizontal pod autoscaling
      - Service discovery
      - Rolling updates

      Example deployment spec provided but marked as beyond MVP.
    delta: |
      Need to:
      - Create basic Kubernetes manifests
      - Test with local Kubernetes (minikube, kind)
      - Document scaling procedures

      Note: Explicitly marked as beyond MVP scope.
    dependencies:
      - GAP-INFRA-001
      - GAP-INFRA-006
      - GAP-INFRA-007
      - GAP-INFRA-008
    complexity: XL
    risk: low
    files_affected:
      - k8s/ (new directory)
      - k8s/deployment.yaml (new)
      - docs/architecture/current/supporting_systems.md
    acceptance_criteria:
      - Basic Kubernetes manifests exist
      - Works with minikube or kind
      - Marked as future/optional in docs

  # ============================================================================
  # SCALING GUIDANCE
  # ============================================================================

  - id: GAP-INFRA-018
    component: infrastructure
    dimension: configuration
    title: "No scaling guidance documentation"
    current_state: |
      No documentation on when to use different deployment
      configurations based on agent count.
    target_state: |
      Scaling guidance:
      - <50 agents: Single container, SQLite ledger
      - 50-200 agents: Single container, consider PostgreSQL
      - 200+ agents: Multiple containers, PostgreSQL + Redis
      - High availability: Kubernetes with PostgreSQL HA
    delta: |
      Need to:
      - Document scaling thresholds
      - Provide configuration examples for each tier
      - Include performance expectations
    dependencies: []
    complexity: S
    risk: low
    files_affected:
      - docs/architecture/current/supporting_systems.md
      - README.md
    acceptance_criteria:
      - Scaling guidance documented
      - Clear thresholds for each tier
      - Example configs for each tier

# Summary Statistics:
# - Total gaps: 18
# - By complexity: S=5, M=7, L=4, XL=2
# - By risk: low=12, medium=5, high=1
# - Critical path: GAP-INFRA-001 (Docker) enables many other gaps
