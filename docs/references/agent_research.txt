Building an Agentic System
https://gerred.github.io/building-an-agentic-system/print.html
Agents
Tuesday, June 3, 2025 12:54 PM
Programming Page 1
Deep Research on AI Agent Methods
1. Introduction: Understanding AI Agents
Artificial Intelligence (AI) agents are software entities designed to autonomously perform tasks or make
decisions based on predefined objectives and data inputs. They exhibit intelligent behavior through
autonomy, reactivity, proactiveness, and social ability, interacting with their environment and users to
achieve specific goals by perceiving inputs, reasoning about tasks, planning actions, and executing them
using internal and external tools.
The evolution of AI agents has seen a significant transformation. Early agent-like systems were primarily
reactive or deliberative, relying on symbolic reasoning, rule-based logic, or scripted behaviors. The
emergence of Large Language Models (LLMs) has fundamentally reshaped agent systems, moving
beyond traditional rule-based agents with limited task scope to offer greater flexibility, cross-domain
reasoning, and natural language interaction. This has led to two distinct yet interconnected paradigms:
standalone AI Agents and collaborative Agentic AI ecosystems.
AI Agents are typically designed as single-entity systems that perform goal-directed tasks by invoking
external tools, applying sequential reasoning, and integrating real-time information. In contrast, Agentic
AI systems are composed of multiple, specialized agents that coordinate, communicate, and dynamically
allocate sub-tasks within a broader workflow, representing a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy.
2. Types of AI Agent Architectures
The AI landscape features several distinct agent architectures, each with its own approach to decision-
making.
2.1. Reactive Agent Architectures
Reactive agents operate through direct stimulus-response mechanisms based solely on the current state
of their environment. They do not maintain internal models or memories of past experiences, relying
instead on predefined rules to generate immediate responses to environmental inputs. Their
architecture follows a straightforward perception-action loop, processing sensor information through
simple rule sets to determine actions. Examples include non-player characters (NPCs) in video games
that require fast, predictable responses.
2.2. Deliberative Agent Architectures
Unlike reactive systems, deliberative agents leverage complex internal models to reason about their
actions and plan for future outcomes. They engage in sophisticated planning processes, evaluating
various scenarios and potential outcomes before committing to actions, similar to how a chess player
thinks several moves ahead. This strategic approach allows them to handle complex tasks requiring long-
term planning and goal achievement, optimizing actions for better long-term outcomes. Applications
include personal digital assistants that manage schedules and prioritize tasks.
2.3. Hybrid Agent Architectures
Hybrid agent architectures combine the strengths of both reactive and deliberative systems, offering
lightning-fast reactive responses while maintaining the ability to plan. They balance immediate
responses to environmental changes (reactive component) with strategic planning capabilities, making
them suitable for complex real-world applications like self-driving cars.
2.4. Utility-Based Agent Architectures
Utility-based agents aim to maximize overall benefits, considering both short-term and long-term
implications of their actions. Unlike simpler agents that react purely based on current conditions, these
agents make decisions by evaluating potential future outcomes to optimize utility.
3. LLM-Powered AI Agents
Agents Methods deep resesearch Openai
Tuesday, June 3, 2025 11:49 PM
Programming Page 2
3. LLM-Powered AI Agents
The advent of Large Language Models (LLMs) has significantly advanced AI agent systems, providing
greater adaptability in dynamic and open environments compared to traditional rule-based or
reinforcement learning agents.
3.1. Capabilities and Integration
LLM-powered agents offer enhanced flexibility, cross-domain reasoning, and natural language
interaction. With the integration of multi-modal LLMs, these agents can process diverse data modalities,
including text, images, audio, and structured tabular data, enabling richer and more adaptive real-world
behavior. They are capable of advanced functions such as natural language understanding, autonomous
problem-solving, planning, reasoning, and human-like interaction, significantly enhancing human-AI
collaboration through context-aware dialogue and real-time decision support.
3.2. Core Components
A typical LLM-powered agent system integrates several critical components :
LLM as the Cognitive Engine: The LLM serves as the core, responsible for high-level reasoning,
planning, and natural language understanding.
•
Tool Utilization: Supporting modules enable the agent to dynamically invoke APIs, databases, or
third-party models to accomplish specialized tasks, often facilitated by techniques like Multi-
Context Prompting (MCP).
•
Memory: Memory systems are typically implemented to manage persistent context and
information.
•
4. Multi-Agent Systems (MAS)
Multi-Agent Systems (MAS) represent a significant advancement, enabling complex problem-solving
through coordinated specialized agents. These systems draw inspiration from human organizational
structures, distributing cognitive labor across multiple agents with specialized capabilities to tackle
problems of greater complexity and scale than single-agent approaches.
4.1. Characteristics and Benefits
MAS feature collections of autonomous agents, each with distinct capabilities, knowledge bases, and
objectives, working together through defined coordination mechanisms. The benefits include:
Flexibility and Scalability: MAS can adapt to changing environments by adding, removing, or
modifying agents, making them highly scalable for complex problems.
•
Robustness and Reliability: Decentralization of control allows continued system operation even if
some components fail, leading to greater robustness and fault tolerance.
•
Self-Organization and Coordination: Agents can self-organize based on emergent behavior rules
for division of labor, coordinated decision-making, and conflict resolution.
•
Real-time Operation: MAS enable immediate situational responses without human oversight,
applicable in areas like disaster rescue and traffic optimization.
•
Collective Intelligence: The combined capabilities of multiple agents can exceed the sum of their
individual contributions, leading to emergent collective intelligence.
•
4.2. Coordination and Communication Mechanisms
Coordination in MAS is crucial for achieving shared goals. Key communication approaches include :
Agent Communication Languages (ACLs): Formalized languages like FIPA-ACL and KQML provide
standardized message structures for sophisticated interaction patterns.
•
Ontology-based communication: Shared ontologies provide common vocabularies and semantic
frameworks for consistent interpretation across agents.
•
Natural language communication: With LLMs, natural language has become a viable medium,
offering flexibility and expressiveness.
•
Protocol-based interaction: Defined interaction protocols specify expected message sequences
for coordination tasks (e.g., contract net protocol).
•
Blackboard systems: Shared information spaces allow agents to post and retrieve information
without direct communication.
•
4.3. Types of Agent Relationships
Programming Page 3
4.3. Types of Agent Relationships
Multi-agent cooperative decision-making involves multiple agents working together to complete tasks
and achieve objectives, applicable in scenarios like autonomous driving and disaster rescue. Agent
relationships can be categorized as :
Fully Cooperative: Agents have aligned objectives and share identical reward structures, working
towards a common goal to maximize collective benefits.
•
Fully Competitive: Characterized by a zero-sum game dynamic where one agent's gain is another's
loss, common in competitive environments like robotic competitions.
•
Mixed Cooperative and Competitive: Agents engage in both cooperation and competition
simultaneously, as seen in team-based environments like robotic soccer where teammates
cooperate but compete against opposing teams.
•
Approaches to multi-agent cooperative decision-making include rule-based, game theory-based,
evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and LLM
reasoning-based methods.
5. Cognitive Architectures for AI Agents
Cognitive architectures encapsulate theories and commitments toward a general, systems architecture
for intelligence. Over a hundred different cognitive architectures have been proposed, showing notable
convergence around high-level functional architectures of cognition. These architectures aim to identify
mechanisms and representations sufficient for general intelligence.
Recurring cognitive design patterns found in pre-transformer AI architectures are now evident in
systems using LLMs for reasoning and interactive use cases. These patterns include:
Observe-decide-act: Seen in BDI (Belief-Desire-Intention) agents (analyze, commit, execute) and
Soar (elaborate/propose, decide, apply operators).
•
3-stage memory commitment: Involves generating candidates for memory, then using a selection
or commitment process to choose, as in BDI (desire, intention, intention reconsideration) and Soar
(operator proposal, selection, retraction).
•
Hierarchical decomposition: Utilized in BDI with hierarchical task networks (HTNs) and in Soar
with operator no-change impasses.
•
Short-term (context) memory: Examples include ACT-R's buffers (goal, retrieval, visual, manual)
and Soar's working memory.
•
Ahistorical and Historical Knowledge Representation (KR)/memory: ACT-R and Soar use semantic
memory for ahistorical KR, while Soar also includes episodic memory for historical KR.
•
Procedural KR/memory: ACT-R and Soar use productions, and BDI uses plans.•
Learning: ACT-R and Soar employ knowledge compilation/chunking.•
The Soar cognitive architecture, for instance, was developed to address limitations in problem-solving by
advancing goal-oriented search through a problem space, incorporating components like reinforcement
learning, impasses, sub-states, and chunking to enhance capabilities and learn from experience.
Similarly, ACT-R is widely used for understanding human cognition and can be integrated with LLMs to
create "Cognitive LLMs" that replicate human-like decision-making by transferring knowledge of
cognitive models' internal processes into LLM adapter layers.
6. Applications of AI Agents
AI agents are being deployed across a wide range of domains, from individual assistance to complex
multi-agent collaborations.
6.1. Individual AI Agent Implementations
Individual AI agents, often enhanced with foundation models, excel in well-defined task domains
requiring specialized knowledge and consistent execution. Applications include:
Customer Service: Advanced customer service systems.•
Content Management: Intelligent document processing platforms.•
Personalized Recommendations: Recommendation engines.•
Automated Workflows: Email filtering, database querying, and calendar coordination.•
Virtual Assistants: General-purpose AI assistants.•
6.2. Collaborative Agentic AI Application Landscapes
Programming Page 4
6.2. Collaborative Agentic AI Application Landscapes
Agentic AI systems, with their multi-agent architectures, are designed for objectives that exceed
individual agent capabilities, demonstrating emergent intelligence through coordination. Applications
span:
Research Automation: Automating literature reviews, data preparation, experimentation, and
report writing in scientific discovery. Systems like Virtual Scientists (VIRSCI) mimic teamwork in
scientific research to collaboratively generate, evaluate, and refine research ideas.
•
Complex Decision Support: Providing support in intricate decision-making scenarios.•
Adaptive Workflow Management: Managing workflows across multiple domains simultaneously.•
Supply Chain Management: Optimizing complex supply chains.•
Business Process Optimization: Streamlining business operations.•
Robotic Coordination: Orchestrating robotic tasks.•
LLM-powered agents are also categorized into software-based (digital environments, APIs), physical
(sensors, actuators in the physical world), and adaptive hybrid systems (combining both for real-world
integration and continuous learning).
7. Challenges and Future Directions
Despite significant advancements, AI agent development faces several challenges and offers numerous
opportunities for future research.
7.1. Technical Challenges
Unpredictability of Multi-step User Inputs: The diversity and potential for inadequate description
or malicious intent in multi-step user inputs can lead to security threats and unintended
consequences.
•
Complexity in Internal Executions: The intricate chain-loop structure of internal execution states,
from prompt reformatting to LLM planning and tool use, makes detailed observation difficult.
•
Variability of Operational Environments: Ensuring consistent behavior across diverse operational
environments is a challenge.
•
Interactions with Untrusted External Entities: The assumption of trusted external entities in
current interaction processes creates attack surfaces, such as indirect prompt injection.
•
High Inference Latency and Operational Costs: LLM-powered agents can incur high operational
costs and latency, particularly for complex tasks.
•
Output Uncertainty and Lack of Evaluation Metrics: There is a need for standardized benchmarks
and metrics to evaluate the performance, safety, and resource needs of AI agents, especially given
the output uncertainty.
•
Hallucination and Shallow Reasoning: Both AI Agents and Agentic AI systems can suffer from
hallucination and shallow reasoning.
•
Lack of Causality: A fundamental challenge in AI agent development.•
Scalability: Scaling AI systems to handle millions of users or devices is challenging due to
infrastructure limitations.
•
7.2. Security and Ethical Concerns
Privacy Vulnerabilities: Delegating tasks to AI agents extends the attack surface, allowing
adversaries to target agents to extract sensitive data or manipulate them for unauthorized
actions.
•
Secret Collusion: Decentralized AI agents can develop covert communication channels,
embedding hidden messages that evade conventional monitoring, raising concerns about
undetected collusion and erosion of AI safety measures.
•
Emergent Biases and Systemic Risks: Biases can emerge from collective interactions in multi-
agent systems, amplifying systemic inequalities in areas like resource allocation and healthcare.
Malicious agents can also exploit fairness mechanisms.
•
Fairness and Non-Discrimination: Ensuring fairness in AI decision-making is a persistent challenge,
requiring mitigation of bias in AI decisions to prevent unfair treatment.
•
Transparency and Explainability: Many AI models, especially deep learning systems, function as
"black boxes," making their decision-making processes opaque. Improving transparency and
•
Programming Page 5
"black boxes," making their decision-making processes opaque. Improving transparency and
interpretability is crucial for trust and adoption, particularly in critical fields. Explainable AI (XAI)
aims to provide understandable and accurate reasons for system outputs.
Robustness and Accuracy: Sustaining a model's performance in unexpected circumstances and
ensuring accuracy are key requirements for trustworthy AI.
•
Accountability: Understanding who is responsible for the decisions of AI systems is a significant
ethical consideration.
•
Human Agency and Oversight: Sustaining the autonomy of humans affected by AI systems is a
critical requirement.
•
7.3. Future Research Directions
Future research will emphasize human-AI collaboration and enhanced system calibration. Bridging the
knowledge gaps in AI agents, such as unpredictability of user inputs, complexity in internal executions,
variability of environments, and interactions with untrusted entities, will lead to improved task
outcomes, enhanced security, consistent behaviors, and increased user trust. Opportunities include:
Explainable and Transparent AI (XAI): Focus on creating models that provide clear and
understandable explanations for their predictions and decisions, especially vital in healthcare,
legal, and financial domains.
•
AI for Sustainability: AI is poised to play a significant role in addressing environmental challenges,
including climate modeling and monitoring.
•
Augmenting Human Expertise: The true power of Agentic AI lies in its ability to augment human
expertise rather than replace it, accelerating scientific discovery and democratizing access to
advanced research tools.
•
Addressing Challenges in Multi-Agent Systems: Continued research on context management,
coordination efficiency, and scalable operation in multi-agent systems, potentially through
frameworks like Model Context Protocol (MCP).
•
Google Account
Brian Mills
brianmills2718@gmail.com
Programming Page 6
LLM Agent Architectures: Comparative Overview
Large Language Model (LLM) agents are systems that use LLMs as their “brain” to reason, plan, and act,
often by invoking tools or external modulesdeveloper.nvidia.commedium.com. Architectures for such
agents range from single-agent designs (one model controlling everything) to multi-agent systems
(multiple specialized models collaborating)medium.commicrosoft.com. These architectures can vary in
modularity (how independently components can be swapped or replaced), observability (how much the
system’s internal reasoning can be monitored), ease of use (how developer-friendly the setup is),
performance, fault tolerance, tool integration, model agnosticism, and cost efficiency. Below we
compare major approaches and frameworks across these dimensions, with examples and references.
Architec
ture /
Framew
ork
Type Modular
ity
Observa
bility
Ease of
Use
Perform
ance
Fault
Toleranc
e
Tool
Integrati
on
Model
Agnostic
ism
Cost
Efficienc
y
Monolit
hic LLM
(ChatGP
T-style)
Single-
Agent
Very
low –
single
model
Very
low –
opaque
reasonin
g
Very high
(simple
to start)
Moderat
e (fast
inference
, limited
reasonin
g)
Low (no
recovery
if model
fails)
Low (no
built-in
tools)
Very
high
(any
LLM)
High
(one
model
call)
LLM +
Tools
(ReAct /
Plugins)
Single-
Agent
Medium
(separat
e model
& tools)
Low–
Medium
(external
actions
logged)
Medium
(requires
coding of
tools)
High
accuracy
(by
specializ
ation),
slower
(extra
calls)
Medium
(tool
errors
possible
but often
catchabl
e)
Very
high
(designe
d for
tool use)
High
(any LLM
with API)
Medium
–Low
(many
calls)
Agent
Framew
orks
(LangCh
ain,
Strands)
Single-
Agent
High
(modula
r
chains/c
ompone
nts)
Medium
(built-in
tracing/l
ogging)
High
(abstract
s
workflow
s)
Good
(with
fine-
tuning),
overhead
for
framewo
rk
Medium
(framew
orks
support
retries/lo
gs)
Very
high
(plug-
and-play
tools)
High
(support
multiple
LLMs)
Medium
(framew
ork
overhea
d)
LLM
Orchestr
ator
(Hugging
GPT)
Multi-
Agent
Very
high
(distinct
models)
Medium
(plans
visible,
model
decisions
hidden)
Low
(complex
setup)
High
(leverage
s
experts)
Medium
(model
failures
need
custom
handling)
Very
high
(invokes
many
ML
models)
High
(coopera
tes
multiple
LLMs)
Low
(many
large-
model
calls)
Multi-
Agent
(AutoGe
n/Magn
etic-
Multi-
Agent
High
(orchest
rator +
specialis
t agents)
Medium
–High
(central
log by
controlle
Medium
(framew
orks
exist, but
design
High
(parallel
tasks,
specializ
ed skills)
High
(controll
er can
re-plan
on
High
(agents
can use
various
APIs/too
High
(framew
orks
support
different
Low
(multipl
e
agents/
models)
LLM Agent Architectures DeepResearch Openai
Wednesday, June 4, 2025 6:24 AM
Programming Page 7
etic-
One)
t agents) controlle
r)
design
work
needed)
ed skills) on
errors)
APIs/too
ls)
different
LLMs)
models)
Hierarch
ical
Multi-
Agent
(Nexus)
Multi-
Agent
Very
high
(supervis
ors +
workers)
Medium
(supervis
or logs
tasks)
Medium
(develop
ers build
hierarchi
cal logic)
Very high
(paralleli
sm,
optimizat
ion)
High
(task
reassign
ment on
failure)
High
(support
s shared
memory,
tools)
High
(flexible
multi-
LLM)
Low
(many
models/
agents)
Decentr
alized
Multi-
Agent
(AgentSc
ope)
Multi-
Agent
Very
high
(peer
agents)
Medium
(messag
e logs
available
)
Medium
(framew
ork
abstracts
messagin
g)
High
(distribut
ed
parallelis
m)
Very
high
(built-in
retries &
controls)
High
(built-in
tool
wrapper
s)
High
(model-
flexible
design)
Medium
(open-
source,
but
parallel)
Table: Comparison of LLM-based agent architectures. Values (Very low/low/medium/high/very high)
indicate relative trade-offs (e.g. Modularity = how easily components can be replaced). See text for
details and sources.
Key Concepts
Modularity: the degree to which an agent system is composed of interchangeable parts. A
monolithic design (one LLM doing everything) has very low modularity, whereas multi-agent or
pipeline systems (with separate planners, memories, tools, etc.) are highly modulararxiv.org. In
modular systems, components (like a planner or tool interface) can be swapped or upgraded
independently.
•
Observability: how much the system’s internal reasoning can be inspected or logged. If only the
LLM’s final output is visible (as in monolithic use), observability is very low; by contrast, agent
frameworks that log each thought, action, and observation provide medium to high observability.
Observability enables debugging and understanding of decisions by analyzing inputs/outputs at
each steparxiv.org.
•
Ease of Use: how straightforward it is for a developer to get started. Simple single-LLM usage (just
call ChatGPT) rates very high (no setup), while multi-agent orchestration frameworks require
more design, giving medium ease-of-use. Many open-source toolkits (LangChain, AWS Strands,
Microsoft AutoGen) improve usability by providing abstractions, improving ease of use for
complex architectures.
•
Performance: measured by task success and speed. Single-LLM agents can answer simple queries
quickly but may struggle on complex tasks. Tools or specialized agents improve accuracy on
complex tasks at the cost of extra latency (multiple model calls or API invocations). Architectures
like Nexus report state-of-the-art results on coding and math benchmarks, showing very high
performance when properly tunedarxiv.org.
•
Fault Tolerance: the ability to handle failures (e.g. model errors or timeouts). Monolithic agents
have low fault tolerance—if the LLM fails or hallucinate, there is no fallback. In contrast,
hierarchical/multi-agent designs can detect failures and reassign or retry tasks. For example,
Microsoft’s Magentic-One orchestrator is designed to “re-plan to recover from errors”cio.com,
and the AgentScope platform provides customizable retry mechanisms to handle diverse LLM/API
failuresarxiv.orgcio.com.
•
Tool Integration: how well the architecture allows using external APIs or functions. Single-LLM
setups have low native support (they just take text), whereas frameworks like Strands explicitly
connect LLM “thoughts” to tool invocationsjtanruan.medium.com. Multi-agent systems can assign
specific agents the role of interfacing with tools (e.g. one agent browses the web, another runs
code)cio.com.
•
Model Agnosticism: whether the design depends on a specific LLM. Architectures that treat the
LLM as a black-box (e.g. via API) tend to be highly model-agnostic. For instance, LangChain and
•
Programming Page 8
LLM as a black-box (e.g. via API) tend to be highly model-agnostic. For instance, LangChain and
AutoGen are designed to work with any LLM backend. By contrast, some specialized platforms
may initially target one model (e.g. GPT-4), but open-source frameworks allow swapping models
relatively easilyarxiv.orgmicrosoft.com.
Cost Efficiency: roughly, how many model/API calls are needed. Monolithic agents (one query) are
cost-efficient, while multi-agent systems incur more calls. Using specialized smaller models (as
Nexus advocates) or running agents in parallel can reduce latency but often requires more total
computemedium.commedium.com. Open-source implementations (like AgentScope, Nexus) may
lower monetary cost by allowing local model use, but the overall cost (compute time, engineering
effort) tends to rise with complexity.
•
Single-Agent Architectures
Monolithic LLM Agent: A single LLM (e.g. ChatGPT) acts as both planner and executor. This is the
simplest setup: prompt the model and use its output directly. Modularity is very low (no separable
parts), and observability is very low since only the final answer is seen. Ease of use is very high
(just one API call)developer.nvidia.commedium.com. Performance on simple tasks is decent, but
this approach struggles with complex, multi-step tasksmedium.com. Fault tolerance is minimal—if
the LLM output is wrong or times out, the system has no built-in recovery. There is essentially no
tool integration. Being just an LLM call, it is highly model-agnostic (you can use any LLM). It is very
cost-efficient (one call per query) but can become expensive for large models.
•
LLM + Tools (ReAct/Plugins): One model generates reasoning steps (“Thought”) and actions
(“Action”) in turnpromptingguide.ai, calling APIs or tools when needed. Architectures like the
ReAct framework or ChatGPT with plugin tools fall in this category. Modularity is medium: the LLM
is separate from tool modules, so components can be swapped. Observability is low–medium: we
can log each action (API call) but still only infer the LLM’s hidden thought process. Ease of use is
medium: setting up tools requires code, though frameworks like AWS Strands simplify
itjtanruan.medium.com. Performance is typically higher on complex tasks (since specialized tools
fill knowledge gaps), but inference is slower due to multiple steps. Fault tolerance is medium: if a
tool call fails or returns unexpected output, the agent can often catch it and retry or choose an
alternate (and some frameworks provide error-handling hooksarxiv.org). Tool integration is very
high by design (these architectures exist to facilitate API calls). Model agnosticism is high, as any
LLM that can follow the ReAct-style prompt can be used. Cost efficiency is medium–low, since
multiple LLM calls and tools increase cost.
•
Agent Frameworks (LangChain, Strands, etc.): These are open-source libraries that help build
single-agent systems. They typically provide high modularity (chains of components, memory
buffers, and tool wrappers) and often have built-in logging or tracing for better
observabilitymedium.com. Ease of use is generally high: they abstract away much boilerplate,
letting developers add tools and memory easily. Performance depends on the underlying design,
but these frameworks usually add some overhead. Fault tolerance can be medium, since they
often include retry logic or human-in-the-loop checkslinkedin.com. They excel at tool integration
(helper functions for APIs, search, code execution) and are model-agnostic (support many LLM
backends). However, using a full framework means more total compute overhead, so cost
efficiency is medium.
•
Example: NVIDIA’s Strands SDK lets you define an agent with a prompt and list of tools; it “connects the
model and the tools” so the LLM decides which API to calljtanruan.medium.com. LangChain similarly
provides preset agent types (e.g. ReActAgent) and memory modules. These frameworks make
development easier but add layers that can affect runtime.
Multi-Agent Architectures
LLM Orchestrator (HuggingGPT style): Here one LLM acts as a controller, decomposing a task and
delegating sub-tasks to other expert models or agentsproceedings.neurips.cc. For example,
Microsoft’s HuggingGPT uses ChatGPT to select from thousands of Hugging Face models to solve
vision, language, or code tasksproceedings.neurips.cc. Modularity is very high (separate controller
and specialists). Observability is medium: the orchestrator’s plan and the chosen model names
can be logged, but the internal workings of each agent remain hidden. Ease of use is low:
•
Programming Page 9
can be logged, but the internal workings of each agent remain hidden. Ease of use is low:
designing the orchestration logic is complex, and coordinating many models takes effort.
Performance can be high: by using the right specialized model for each sub-task, the system can
solve very complex tasks that a single LLM cannot. Fault tolerance is medium: if a specialist fails,
the orchestrator could try an alternative or catch errors, but this requires extra logic. Tool
integration is very high in spirit (the specialists are essentially tools/models). Model agnosticism is
high at the controller level (it could orchestrate different models), but each specialized model
choice can be fixed. Cost efficiency is low, since multiple large models may be called in sequence.
Multi-Agent Conversation (AutoGen, Magentic-One): These frameworks allow multiple agents
(often all LLM-based) to talk to each other in a controlled way. One agent (a supervisor or
orchestrator) decomposes tasks, and other agents (workers) execute themmicrosoft.comcio.com.
Magentic-One (Microsoft) for instance has one Orchestrator and four specialized agents
(WebSurfer, FileSurfer, Coder, Terminal) that browse the web, read files, write code, etc. The
orchestrator “plans, tracks progress, and re-plans to recover from errors”cio.com. Modularity is
high (clear roles for each agent). Observability is medium–high: the orchestrator’s reasoning and
chat logs can be recorded. Ease of use is medium: frameworks like Microsoft’s AutoGen provide
templates, but designing agent roles requires work. Performance is high: parallel or sequential
specialized processing can outperform a single agent on complex tasksmicrosoft.comcio.com.
Fault tolerance is high: since the supervisor can detect failures and reassign tasks, the system can
recover from a worker agent’s error. Tool integration is high: each agent can have its own
tools/APIs (e.g. a code-running agent). Model agnosticism is high, as most frameworks allow
plugging in any LLM for each agent. Cost efficiency is low (multiple agents and back-and-forth
conversation means many model calls).
•
Example: Microsoft’s AutoGen is an open-source multi-agent framework where developers “compose
multiple agents to converse with each other to accomplish tasks”microsoft.com. The Magentic-One
system uses this idea: its Orchestrator (based on GPT-4o) coordinates four other agents with specialized
functionscio.com.
Hierarchical Multi-Agent (Nexus): This design uses a hierarchy of agents, often with one or more
supervisors above worker agents. Nexus (Sami et al., 2025) is an academic framework where a
global supervisor decomposes tasks and assigns workers, who then iteratively use tools and share
results via a common memoryadasci.orgarxiv.org. Modularity is very high (layers of agents).
Observability is medium: the supervisor can log plans and workers’ outputs (a shared memory can
be inspected)adasci.org. Ease of use is medium: Nexus provides a Python library and pip-
installable package for building hierarchical agentsarxiv.org, but understanding the loops can be
complex. Performance is very high: in experiments, Nexus-based architectures achieved near-
perfect scores on coding and math benchmarksarxiv.org. Fault tolerance is high: the hierarchy can
reassign subtasks if a worker fails, and redundant review agents can catch errors. Tool integration
is high: Nexus explicitly supports shared memory stores and external APIsadasci.org. Model
agnosticism is high (any LLM or combination can be used in supervisor/workers). Cost efficiency is
low, since many LLM calls occur, though Nexus argues that using smaller models in workers can
mitigate overall costarxiv.orgmedium.com.
•
Decentralized Multi-Agent (AgentScope): Here many peer agents communicate via messaging
rather than a single controller. AgentScope (Alibaba) is a recent open-source platform for such
systemsarxiv.orgarxiv.org. It features a message-exchange core so agents can directly collaborate.
Modularity is very high (each agent is independent). Observability is medium: messages can be
logged and a central monitor may track agent states, but there is no single brain to inspect. Ease
of use is medium: AgentScope provides utilities (tools, templates) to speed development, but
designing multi-agent logic remains challenging. Performance is high due to parallelism. Fault
tolerance is very high: AgentScope includes built-in and customizable retry mechanisms to handle
LLM or tool failuresarxiv.org, and an actor-based distributed mode automatically handles node
failuresarxiv.org. Tool integration is high (it includes syntactic tool interfaces and storage). Model
agnosticism is high (works with open-source or API models)arxiv.org. Cost efficiency is medium:
being open-source helps, but running many agents in parallel still demands resources.
•
Example: The AgentScope paper notes that multi-agent development is complex, so they built a
Programming Page 10
Example: The AgentScope paper notes that multi-agent development is complex, so they built a
“developer-centric” platform with communication mechanisms and “abundant syntactic tools”arxiv.org.
It explicitly provides robust fault tolerance (retries, configurable error handlers) to keep agents running
smoothlyarxiv.org.
References
Key principles and examples were drawn from recent literature and documentation. For instance, Adimi
et al. (2025) contrast single-agent vs multi-agent architecturesmedium.commedium.com. Park et al.
(2024) propose a modular “von Neumann”–inspired architecture for LLM agentsarxiv.org. NVIDIA’s blog
defines LLM agents as systems with planning, memory, and tool usedeveloper.nvidia.com. The
AgentOps survey discusses observability needs for agentsarxiv.org. Microsoft’s AutoGen paper and blog
describe conversational multi-agent designmicrosoft.com, and the HuggingGPT paper details
orchestration of many specialized modelsproceedings.neurips.cc. Alibaba’s AgentScope provides an
example of a distributed fault-tolerant platformarxiv.orgarxiv.org. These and other sources (cited above)
illustrate how design choices trade off modularity, performance, ease-of-use, and cost in LLM agent
systems.
Programming Page 11
Comparison of Major Non-LLM AI Agent Types
Agent
Type
Core
Archite
cture/F
ormalis
m
Learnin
g
Paradig
m
Cogniti
ve
Modeli
ng
Strengt
hs
Weakn
esses
Typical
Use
Cases
Interpr
etabilit
y
Scalabil
ity
Real-
Time
Adapta
bility
Key
Exampl
es/Fra
mewor
ks
Historic
al
Context
Symbol
ic
(GOFAI)
Logic/r
ule-
based
(first-
order
logic,
semanti
c nets,
frames)
geeksfo
rgeeks.
org
None
(hand-
crafted
rules/K
Bs)
geeksfo
rgeeks.
org
Low
(focus
on
formal
reasoni
ng)
Transpa
rent,
logical
reasoni
nggeek
sforgee
ks.org;
excels
on
structur
ed
expert
tasks
Poor
scalabili
ty (rule
explosi
on)
geeksfo
rgeeks.
org;
brittle
with
ambigui
ty or
novel
datage
eksforg
eeks.or
g
Expert
systems
(medica
l
diagnos
is e.g.
MYCINg
eeksfor
geeks.o
rg, legal
reasoni
ng),
symboli
c
plannin
g
Very
high
(explicit
rule
trace)
geeksfo
rgeeks.
org
Lowgee
ksforge
eks.org
(expan
ds
poorly)
Low
(static
knowle
dge; no
online
learnin
g)
geeksfo
rgeeks.
org
Prolog,
CLIPS,
Drools,
MYCIN
Domina
nt in
early AI
(1950s–
80s);
decline
d with
rise of
ML in
1990sg
eeksfor
geeks.o
rg; now
used in
neurosy
mbolic
hybrids
Reactiv
e/Beha
vioral
(Subsu
mption)
Layered
sensori
motor
behavio
rs
(augme
nted
FSMs)
en.wiki
pedia.o
rg
None
(predefi
ned
behavio
rs)
Very
low
(purely
reactive
)
Fast,
robust
real-
time
reactio
nen.wik
ipedia.o
rgen.wi
kipedia.
org;
decentr
alized
parallel
control
No
memor
y or
global
modele
n.wikip
edia.or
g;
cannot
learn
comple
x tasks
or plan
ahead
Mobile/
robotic
control
(obstacl
e
avoidan
ce,
navigati
on)
Modera
te
(simple
rules
are
underst
andable
)
Good
(layers
can be
added)
Excelle
nt
(design
ed for
immedi
ate
respons
e)
Brooks’
subsum
ption
robots
Introdu
ced
mid-19
80s as
“Nouve
lle AI”
alternat
ive to
symboli
c
AIen.wi
kipedia.
org
BDI
(Belief-
Desire-
Intenti
on)
Modal
logic
model
with
explicit
beliefs,
desires
(goals),
intentio
nsen.wi
None
(archite
ctural;
plans
are
predefi
ned)
en.wiki
pedia.o
rg
Modera
te
(model
s goal-
driven
behavio
r)
Natural
goal/pl
an
structur
e; clear
“menta
l” state
represe
ntation
No
built-in
learnin
g or
lookahe
aden.wi
kipedia.
org;
require
s
Agent-
oriente
d
progra
mming
(virtual
assistan
ts,
game
AI,
High
(interna
l states
explicit)
Modera
te
(multi-
agent
settings
)
Low
(mainly
deliber
ative/pl
anned
respons
e)
PRS,
JACK,
AgentS
peak/Ja
son,
3APLen
.wikipe
dia.org
Formali
zed in
1980s
(Bratm
an et
al.); still
used in
multi-
agent
systems
Non-LLM Agent Types Openai DeepResearch
Wednesday, June 4, 2025 6:25 AM
Programming Page 12
nsen.wi
kipedia.
org
rg s
comple
x plan
librarie
s
AI,
simulati
ons)
systems
and
agent
progra
mming
Reinfor
cement
Learnin
g
Markov
Decisio
n
Process
(trial-
and-
error
with
reward)
en.wiki
pedia.o
rg
Yes
(policy
optimiz
ation
via
reward
s)
Low
(behavi
orist
learnin
g)
Learns
from
interact
ion;
handles
sequen
tial
decisio
n
taskssy
nopsys.
com
Data-
intensiv
e; slow
(needs
many
trials)
synopsy
s.com;
often
opaque
decisio
n
rulessy
nopsys.
com
Control
systems
,
robotic
s, game
AI
(Atari,
Go),
resourc
e
manage
ment
Low
(especi
ally
deep
nets)
synopsy
s.com
High
(with
functio
n
approxi
mators,
but
comput
e-
heavy)
Modera
te (can
adapt
but
needs
retraini
ng)
Q-
learnin
g, DQN,
PPO,
AlphaG
o
Roots
in
1950s
psychol
ogy
(trial-
and-
error);
surged
with
deep RL
breakth
roughs
in
2010s
Evoluti
onary
Algorit
hms
Populat
ion-
based
search
(geneti
c
encodin
gs,
operato
rs)
larksuit
e.com
Yes
(selecti
on,
crossov
er,
mutatio
n)
Low
(evoluti
onary
metaph
or)
Robust
global
search;
paralleli
zablelar
ksuite.c
om
Very
comput
e-
intensiv
e
(many
evaluati
ons)
larksuit
e.com;
converg
ence
not
guarant
eed;
parame
ter-
sensitiv
e
Optimiz
ation
and
design
(schedu
ling,
networ
k/neura
l
evoluti
on)
Low
(solutio
ns are
comple
x/geno
mic)
Modera
te
(scales
with
comput
e
resourc
es)
Low
(batch/
offline
evoluti
on)
Genetic
Algorith
ms
(GA),
NEAT,
CMA-ES
Develo
ped
1960s–
70s
(Hollan
d’s GA);
widely
used in
optimiz
ation
and
creative
design
Swarm
Intellig
ence
Decentr
alized
multi-
agent
(e.g.
ant
colony,
particle
swarm)
en.wiki
pedia.o
rg
No
explicit
learnin
g
(simple
local
rules)
Low
(emerg
ent
group
behavio
r)
Highly
robust
and
scalable
(many
simple
agents)
larksuit
e.come
n.wikip
edia.or
g
Can
premat
urely
converg
e;
emerge
nt
results
may be
unpredi
ctable
Distribu
ted
optimiz
ation
(TSP,
routing)
, swarm
robotic
s,
sensor
networ
ks
Modera
te
(agent
rules
are
simple,
global
outcom
e
comple
x)
High
(adds
agents)
Yes
(agents
act
concurr
ently)
ACO
(Ant
Colony)
, PSO
(Particl
e
Swarm)
, Boids
Introdu
ced
1989 by
Beni &
Wange
n.wikip
edia.or
g;
popular
for
optimiz
ation
and
collecti
Programming Page 13
collecti
ve
robotic
s
Plannin
g
(Symbo
lic)
Logic-
based
plannin
g/searc
h
(STRIPS,
PDDL)
en.wiki
pedia.o
rg
No
(uses
explicit
domain
model)
Low
(algorit
hmic
search)
System
atically
finds
action
sequen
ces;
goal
guarant
ees if
model
is
correct
Require
s
comple
te
model;
combin
atorial
explosi
on;
inflexibl
e to
surprise
s
Task
sequen
cing in
robotic
s,
logistics
,
automa
ted
schedul
ing
High
(plans
are
explicit)
Low
(state
space
grows
combin
atoriall
y)
Low
(mostly
offline
plannin
g)
STRIPS/
GraphPl
an,
FastDo
wnwar
d, FF
planner
Researc
h since
1960s
(Shakey
);
mature
d with
1990s
PDDL
standar
d; still
used in
robot/t
ask
plannin
g
Hybrid
Cogniti
ve
Archite
ctures
Integrat
ed
symboli
c/sub-
symboli
c
architec
tures
(e.g.
ACT-R,
Soar)
en.wiki
pedia.o
rg
Often
include
s
learnin
g (rule
compila
tion,
memor
y)
High
(model
s
human
cognitio
n)
Broad
cognitiv
e
capabili
ties
(reason
ing,
memor
y,
learnin
g)
Very
comple
x;
comput
ationall
y
heavy;
limited
real-
time
through
put
Cogniti
ve
modeli
ng,
intellige
nt
tutorin
g, high-
level
agent
control
Modera
te
(some
module
s
inspect
able)
Low
(design
ed for
cognitiv
e
fidelity)
Low
(simula
tion-
oriente
d)
Soar,
ACT-R,
CLARIO
N, LIDA
Propos
ed
1980s–
90s
(Newell
’s
unified
theory);
active
in AI
and
cognitiv
e
science
researc
h
Each agent type above is described by its core design and computational paradigm, along with how it
learns (if at all) and whether it aims to mimic human-like cognition. Strengths and weaknesses are
summarized, citing relevant literature. Interpretability refers to how easily a human can understand the
agent’s decision process (with symbolic and BDI approaches being highly interpretable, and deep RL or
evolved solutions generally less so). Scalability refers to practical performance on larger problems, and
real-time adaptability indicates whether the agent can adjust on the fly (e.g. reactive/swarm agents
excel here). Representative systems and historical notes illustrate each approach’s
contextgeeksforgeeks.orggeeksforgeeks.orgsynopsys.comen.wikipedia.org.
Sources: Authoritative AI texts and surveys were used to compile this comparison (citations above
include overviews of symbolic AIgeeksforgeeks.orggeeksforgeeks.org, RLsynopsys.comen.wikipedia.org,
BDIen.wikipedia.orgen.wikipedia.org, evolutionary/swarm methodslarksuite.comen.wikipedia.org,
planningen.wikipedia.org, and cognitive architecturesen.wikipedia.org). These sources discuss the
formal models, capabilities, and historical evolution of each agent type.
Programming Page 14
AI Agent Frameworks and Platforms
https://chatgpt.com/c/683b4b61-7b5c-800b-ac47-051f08fcc16c
Project /
System
Link /
Demo
Langua
ge
Open-
Source?
Supported Tools & Capabilities Domain Status
LangChain
kdnuggets.
combotpr
ess.com
GitHub Python Open
(MIT)
LLM-based agents with tool
integration (APIs, databases, search
engines), memory (short/long-term),
RAG support, and “React”-style
agentsbotpress.com; very flexible
toolkit of chains and tools.
General-
purpose
(LLM
apps)
Active
LangGraph
(LangChain
)
GitHub Python Open
(Apache-
2)
Low-level orchestration for stateful,
long-running agents; durable
execution (automatic recovery),
human-in-the-loop oversight,
comprehensive memory (short- and
long-term), plus full integration with
LangSmith for debugginggithub.com.
General-
purpose
(persisten
t agents)
Active
OpenAI
Agents
SDK
GitHub Python Open
(MIT)
Provider-agnostic multi-agent
workflows; LLM “Agents” configured
with instructions, tools, guardrails
and handoffsgithub.com. Supports
function-calling tools, human
handoff, tracing/debugging of agent
runs.
General-
purpose
(multi-
agent)
Active
Microsoft
AutoGen
GitHub Python Open
(MIT/CC-
BY-4.0)
Multi-agent framework with
structured message-passing; agents
(e.g. “Assistant”, “UserProxy”) can
collaborate and call extensions.
Supports tool access including web
browsing, code execution and local
toolsgithub.com. Includes AutoGen
Studio (no-code GUI) and Bench
(evaluation).
General-
purpose
(multi-
agent)
Active
CrewAI GitHub Python Open
(MIT)
Lightweight multi-agent framework
with role-based crews and event-
driven flowskdnuggets.com. Agents
share memory, can run in parallel,
and integrate with custom tools/APIs
(e.g. vector stores, web APIs).
Emphasizes collaboration and
observability.
General-
purpose
(multi-
agent)
Active
PraisonAI GitHub Python Open
(MIT)
End-to-end multi-agent system (built
on AutoGen/CrewAI) with self-
General-
purpose
Active
AI Frameworks and Applications DeepResearch Openai
Wednesday, June 4, 2025 6:26 AM
Programming Page 15
(MIT) on AutoGen/CrewAI) with self-
reflection, reasoning and memory.
Bundles 100+ tools and supports
LangChain agents. Features include:
document/RAG agents, code
interpreter agents, PDF ingestion,
math agents, async/parallel
execution, callback
toolsgithub.comgithub.com.
purpose
(multi-
agent)
SmolAgen
ts
GitHub Python Open
(Apache-
2)
Minimalistic agent library ("agents
that think in code"). Agents write
Python code to call tools and
orchestrate other agents. Supports
many LLMs (OpenAI, HF, etc.) and
focuses on code agents. Lightweight,
aimed at education and
experimentationkdnuggets.com.
General-
purpose
(learning)
Active
Haystack
(by
deepset)
GitHub Python Open
(Apache-
2)
Production-ready LLM orchestration
framework. Pipelines for RAG,
document/question answering,
semantic search. Connects LLMs,
vector DBs (Elasticsearch, Pinecone,
etc.), file converters. Customizable
QA agents over any datagithub.com.
General
(RAG/QA)
Active
Microsoft
Semantic
Kernel
GitHub Python,
C#
Open
(MIT)
Orchestration/“planner” framework.
Breaks user input into multi-step
plans, uses embedding-based
memory and connectors to data
sourcesbotpress.com. Supports
“skills” (modular LLM+API units),
long-term memory. Enterprise-
focused (C#/.NET support).
General-
purpose
(enterpris
e)
Active
AutoGPT
(Significan
t Gravitas)
GitHub Python Open
(MIT)
GPT-4 autonomous agents with self-
planning: agents recursively think of
tasks towards a goal. Out-of-the-box
support for browser search, file I/O,
code execution via plugins. Maintains
vector memory of factsbotpress.com.
Demonstrates minimal supervision
autonomybotpress.com.
General-
purpose
Active
(experim
ental)
AgentGPT
(Reworkd.
AI)
Demo JavaScri
pt
Closed
(propriet
ary)
Browser-based no-code AutoGPT
implementation. Allows users to
deploy GPT-3.5/4 agents by
specifying goals. Likely supports
internet search, browsing, tools (code
execution, calculators, etc.) via
plugins. (General-purpose, beta)
huggingface.co.
General-
purpose
Beta
(online)
SuperAGI GitHub Python Open
(MIT)
Dev-first autonomous agent
framework. Build and run concurrent
agents with state. Supports tool
plugins (browser, search, APIs,
General-
purpose
Active
Programming Page 16
plugins (browser, search, APIs,
message queues). Agents can
continually improve via feedback.
Designed for real-world tasks and
pipelinesgithub.comgithub.com.
Aider GitHub Python Open AI pair-programming CLI. Agents edit
code in local Git repos with GPT.
Supports -run, -fix, -chat commands
to use code-generation tools, run
tests, and commit changes.
Integrates with Git tools for
diff/undo; uses ChatGPT/GPT-4.
Software
developm
ent
Active
Autonomo
us HR
Chatbot
GitHub Python Open HR-focused RAG agent (built on
LangChain). Ingests HR documents
into Pinecone vector DB and uses
ChatGPT/GPT-3.5-turbo. Tools
include embedded policy documents,
a Python code interpreter, and a
calculatorgithub.com. Answers
employee queries on HR policies via
tool use.
Domain-
specific
(HR)
Active
MetaGPT GitHub Python Open
(Apache-
2)
Multi-agent software development
framework. Models a “software
company” where agents handle user
requirements, design, coding, testing.
Agents coordinate via LLM chat;
automates tasks like writing user
stories, generating APIs,
documentation, and code. (Research-
oriented; first AI “company”
prototypegithub.com.)
Domain-
specific
(software
dev)
Active
Sources: Author’s survey of frameworks and
demoskdnuggets.combotpress.comgithub.comgithub.comgithub.combotpress.com, etc. Each row cites
relevant descriptions from project docs or articles.
LLM Frameworks for Agentic and Modular Codebases
Agentic AI Frameworks & Libraries (2025)
https://chatgpt.com/c/68384d39-c7ac-800b-80cd-9dcc64607d6b
Name
(Link)
Purpose
/
Feature
s
LLM
Provide
rs/Mod
els
Agentic
?
RAG? KG? Local? API
Abstrac
tion
Langua
ge
Links
(GitHub
/Docs)
Notes
LangCh
ain
(with
LangGr
High-
level
“chain”
library
OpenAI
(GPT-4,
GPT-4o,
GPT-3.5
Y Y Y Y High Python/
JS
GitHub,
LangGr
aph
Largest
ecosyst
em of
tools
Programming Page 17
LangGr
aph)
apipie.a
i
library
for LLM
workflo
ws
(prompt
s,
memory
, tools).
Massive
compon
ent
ecosyst
em for
agents,
chaining
, and
data
connect
orsapipi
e.ai.
Graph-
based
mode
(LangGr
aph) for
determi
nistic
DAG
workflo
ws.
GPT-3.5
),
Anthrop
ic,
Hugging
Face
(Llama,
BLOOM,
etc.),
Azure
OpenAI,
Google
Vertex
AI, etc.
tools
and
integrat
ions.
Multi-
memory
support,
checkpo
inting.
Used in
product
ion by
many
compan
iesapipi
e.ai.
Haystac
kinfow
orld.co
minfow
orld.co
m
Modula
r
pipeline
framew
ork for
RAG,
QA,
search
and
agentic
workflo
ws.
Support
s
docume
nt
stores,
retrieve
rs
(BM25,
DPR),
and
generat
ors.
Hugging
Face,
OpenAI,
Cohere,
Google
Vertex,
SageMa
ker,
Azure,
etc.;
integrat
es with
vector/
docume
nt
stores
(Elastics
earch,
OpenSe
arch,
Pinecon
e,
Qdrant,
Chroma
Y Y Y Y High Python Docs,
GitHub
“Easy
open-
source
framew
ork for
RAG
pipeline
s”
infoworl
d.com.
Works
with
many
ML
platfor
ms and
vector
DBsinfo
world.c
om.
Offers
agent
compon
ents
Programming Page 18
ors.
Built for
producti
on
QA/assi
stant
appsinf
oworld.
com.
Chroma
,
Weaviat
e,
Neo4j,
etc.)
infoworl
d.com.
ents
and
custom
pipeline
s.
CrewAI
apipie.a
i
Multi-
agent
orchestr
ation
with a
“crew”
metaph
or.
Define
roles
(e.g.
Researc
her,
Writer,
Critic)
that
collabor
ate on
tasksapi
pie.ai.
Support
s self-
organizi
ng or
scripted
flows.
OpenAI
(GPT-4),
Anthrop
ic, likely
local HF
models.
Y Y N Y High Python GitHub Intuitive
role-
based
team of
agents;
built-in
memory
module
s. Self-
organizi
ng
crews
or
explicit
flowsapi
pie.ai.
Popular
(30k+
stars).
AG2
(AutoG
en)
apipie.a
i
Convers
ation-
centric
multi-
agent
framew
ork.
Specializ
ed
agents
(assista
nt, user,
tools)
commu
nicate
asynchr
onously
to solve
Primaril
y
OpenAI
models
(GPT);
extensib
le to
others
via
custom
wrapper
s.
Y Y N Y High Python GitHub “Frames
everythi
ng as
convers
ation
among
specializ
ed
agents”
apipie.a
i. Good
for
parallel
or
multi-
step
workflo
ws;
Programming Page 19
to solve
complex
tasksapi
pie.ai.
Event-
driven
architec
ture.
ws;
support
s
human-
in-loop
feedbac
k.
OpenAI
Agents
SDKapi
pie.ai
Official
OpenAI
agent
framew
ork
(functio
n-
calling).
Focus
on
structur
ed
workflo
ws,
guardrai
ls, and
agent-
to-agent
handoff
sapipie.
ai.
OpenAI
only
(GPT-4,
GPT-4o,
etc.)
Y N N N High Python GitHub Simplifi
ed API
for
managi
ng
prompt
s,
function
schema
s,
callback
s.
Emphasi
zes
product
ion
readine
ss with
filtering
and
validati
onapipi
e.ai.
Google
ADK
(Agent
Dev Kit)
apipie.a
i
Agent
workflo
w SDK
with
explicit
Sequent
ial,
Loop,
Parallel
constru
ctsapipi
e.ai.
Integrat
es
Google
Cloud
tools
and
OpenAP
I specs.
Google
Vertex
AI
models;
generic
tools/A
PIs via
OpenAP
I.
Y N N N Mediu
m
Python GitHub Used
internall
y for
Google
“Gemini
” apps.
A2A
protoco
l for
agent
teams;
secure
GCP
integrat
ion.
Best for
GCP-
centric
enterpri
se
agentsa
pipie.ai.
Programming Page 20
pipie.ai.
Semant
ic
Kernel
(Micros
oft)
apipie.a
i
Plugin/s
kill-
based
agent
framew
ork.
“Skills”
(LLM or
native
function
s) can
be
chained
by a
planner
apipie.ai
.
Support
s C#,
Python,
Java.
OpenAI,
Azure
OpenAI,
Hugging
Face,
local
models
via
Azure/T
ensorRT
, etc.
Y N N Y Mediu
m
C#,
Python
GitHub Convent
ional
progra
mming
approac
h to AI.
Rich
memory
types.
Multi-
languag
e and
Azure-
friendly.
Skills
encoura
ge
modular
designa
pipie.ai.
Huggin
gFace
SmolAg
entsapi
pie.ai
Minimal
“reflexiv
e” agent
framew
ork (~
1000
LOC).
Agents
write
and
execute
Python
code to
use any
librarya
pipie.ai.
Hugging
Face
Hub
models
(Llama,
etc.)
Y N N Y Low Python GitHub Ultra-
simple;
agent
“thinks
in code”
and can
import
any
Python
library.
Useful
for
quick
prototy
ping
and
researc
hapipie.
ai.
LlamaIn
dex
(GPT-
Index)
apipie.a
i
Data-
centric
RAG
framew
ork.
Provide
s
various
index
types
(vector,
keywor
OpenAI,
any LLM
for
embedd
ings (HF
models
like
Sentenc
e-
BERT/Ll
ama2
for
N Y Y Y Mediu
m
Python GitHub “Best-
in-class
RAG”
for
custom
dataapi
pie.ai.
Built-in
knowle
dge-
graph
index
Programming Page 21
keywor
d,
graph)
and
query
engines
apipie.ai
.
Focused
on
connect
ing
LLMs to
private
data.
for
embedd
ing/LLM
)
index
type.
Simplifi
es
ingestio
n/chunk
ing and
multi-
modal
indexing
.
Pydanti
c
AIapipi
e.aiapip
ie.ai
Schema
-driven,
type-
safe
LLM
interfac
e.
Enforce
s output
format
via
Pydanti
c,
reducin
g
parsing
errorsap
ipie.ai.
OpenAI,
Anthrop
ic,
Google,
Hugging
Face,
etc.
N N N Y High Python GitHub Strict
output
schema
s
(typed)
for
reliabilit
y. Multi-
provide
r
support
(OpenAI
,
Anthrop
ic,
Google,
etc.)
apipie.a
i.
Integrat
es with
Logfire
for
monitor
ing.
Agno
(Phidat
a)
apipie.a
i
Ultra-
efficient
agent
framew
ork.
Multim
odal
chain-
of-
thought
;
extreme
ly
Any
(local/H
F LLMs
or API
models
via
Python
SDK)
Y Y N Y High Python GitHub Prioritiz
es
concurr
ency
and
speed
(50×
less
memory
than
LangGra
ph)
apipie.a
Programming Page 22
ly
lightwei
ght (fast
startup,
low
memory
)
apipie.ai
.
apipie.a
i. Built-
in
support
for text,
images,
audio.
Auto-
generat
es
FastAPI
endpoin
ts.
Guidan
ce
(NVIDI
A)
github.c
om
DSL for
structur
ed
prompti
ng.
Control
output
with
selects/
regex/C
FGs,
loops,
conditio
nals,
tool
callsgith
ub.com.
Reduces
cost/lat
ency of
LLM
calls.
OpenAI,
Hugging
Face
(Transfo
rmers),
Llama.c
pp,
VertexA
I, etc.
N N N Y High Python GitHub “Guidan
ce is an
efficient
progra
mming
paradig
m for
steering
LMs”
github.c
om.
Interlea
ves
code
and
generati
on;
support
s
constrai
nt
decodin
g and
stateful
control.
LiteLLM
github.c
om
SDK +
proxy
server
to
host/sw
itch
betwee
n many
LLMs
(cloud
or
local).
Python
gateway
for >100
OpenAI
(GPT-4o
,
GPT-4),
Anthrop
ic,
Azure,
Vertex,
Bedrock
,
Hugging
Face,
Togethe
rAI,
etc.gith
N N N Y High Python GitHub Unified
“OpenA
I-
format”
API for
all
provide
rs.
Streami
ng
support
for all
models
github.c
om.
Programming Page 23
for >100
model
endpoin
ts
(OpenAI
, Azure,
Vertex,
Anthrop
ic, HF,
etc.)
github.c
om.
etc.gith
ub.com
om.
Enables
easy
model
swappin
g and
private
hosting.
DSPy
(Stanfor
d)
github.c
om
Declarat
ive
framew
ork for
“progra
mming”
LLMs
(not raw
prompts
).
Modula
r
pipeline
s; auto-
optimiz
es
prompts
;
support
s self-
improvi
ng
module
sgithub.
com.
OpenAI,
Hugging
Face,
etc.
Y Y N Y High Python GitHub “Iterate
fast on
modular
AI
systems
” with
optimiz
ation
algorith
ms.
Useful
for RAG
pipeline
s and
agent
loopsgit
hub.co
m;
strong
commu
nity
docs.
LMQL
(ETH)
github.c
om
Domain
-specific
languag
e for
LLM
queries.
Allows
Python-
like
control
flow,
constrai
nts, and
type-
checked
generati
OpenAI,
Azure
OpenAI,
Hugging
Face
Transfor
mersgit
hub.co
m
N N N Y High Python GitHub Mixes
code
and
LLM
calls
natively
.
Support
s multi-
model
(API and
HF)
github.c
om.
Good
for
Programming Page 24
generati
ongithu
b.com.
for
schema-
safe
output,
constrai
ned
decodin
g,
complex
prompti
ng.
NVIDIA
AgentI
Qdevel
oper.nv
idia.co
m
Toolkit
for
connect
ing,
evaluati
ng and
optimizi
ng
teams
of AI
agentsd
evelope
r.nvidia.
com.
Provide
s config
builders
, RAG
blueprin
ts,
profiling
/metrics
for
agent
pipeline
s.
Integrat
es with
OpenAI,
Anthrop
ic, HF,
etc. (via
connect
ors)
Y Y N Y Mediu
m
Python GitHub Focus
on full-
stack
agentic
systems
.
Metrics
/teleme
try for
agent
workflo
ws;
includes
RAG
architec
tures
and
blueprin
t
recipesd
evelope
r.nvidia.
com.
Graphit
i (Zep)
github.c
om
Framew
ork for
real-
time,
tempor
al
knowled
ge
graphs
for LLM
agentsgi
thub.co
m.
Continu
ously
integrat
– (no
LLM
API;
used as
memory
layer)
Y Y Y Y Mediu
m
Python GitHub Builds/u
pdates
a
knowle
dge
graph of
agent
interacti
ons and
datagith
ub.com.
Enables
state-
based
reasoni
ng and
Programming Page 25
integrat
es new
info into
evolving
graph;
support
s
efficient
queryin
g.
ng and
graph-
based
retrieval
. Core
of Zep
memory
.
Modusf
osdem.
org
Open-
source
GraphQ
L API
framew
ork with
AI
agents
(WebAs
sembly-
based)
fosdem.
org.
Auto-
generat
es
GraphQ
L
endpoin
ts from
types;
sandbox
ed,
high-
perform
ance.
Open-
source
LLMs
via
WASM
(e.g.,
LLaMA
in
WASM)
Y Y Y Y High Go/Ass
emblyS
cript
GitHub,
Docs
“Intellig
ent
GraphQ
L APIs
powere
d by
LLMs”
fosdem.
org.
Support
s LLM
tool use
and
knowle
dge-
graph
RAG.
Scales
sub-
second
with
WASM
isolatio
n.
Weavia
tedatac
amp.co
mdatac
amp.co
m
Vector
DB with
built-in
knowled
ge
graph
schema.
Stores
objects
+
embedd
ings,
support
s
classes/
relation
sdataca
OpenAI,
Cohere,
Hugging
Face (as
embedd
ings),
plus
custom
vectors.
N Y Y Y High Python
(client)
GitHub Graph-
aware
vector
search.
Fast
nearest-
neighbo
r
searchd
atacam
p.com;
support
s multi-
modal
data
and
semanti
Programming Page 26
sdataca
mp.com
.
Module
s for
semanti
c search
(OpenAI
,
Cohere,
HF, etc.)
dataca
mp.com
.
semanti
c
operati
ons.
Integrat
es
recomm
endatio
n/summ
arizatio
n
module
s.
Chroma
trychro
ma.com
Open-
source
“AI
applicati
on
databas
e” for
RAGtryc
hroma.c
om.
Provide
s
embedd
ings,
fast
vector
search,
full-text
and
metadat
a
filtering,
multi-
modal
support.
Any
embedd
ing
model
(HF,
OpenAI,
etc.)
N Y N Y High Python/
JS
Docs,
GitHub
“Batteri
es
include
d”: an
end-to-
end
vector
index+D
Btrychr
oma.co
m.
Widely
used as
a drop-
in for
LangCha
in/Llam
aIndex.
Focus
on scale
(Chrom
a Cloud
forthco
ming).
Sources: Authoritative project docs and reviews (e.g., LangChain/LangGraphapipie.ai; CrewAIapipie.ai;
Haystackinfoworld.cominfoworld.com; Guidancegithub.com; Graphitigithub.com; etc.). Each entry cites
relevant descriptions from the literature above.
Programming Page 27
Cutting-Edge Agent Reasoning: A Synthesis of Novel
Approaches from Atom of Thoughts, AlphaEvolve, and
Darwin Gödel Machine
Abstract
The landscape of artificial intelligence is undergoing a profound transformation, driven by the rapid
advancements in large language models (LLMs) and their integration into sophisticated agentic systems.
This report provides a comprehensive examination of cutting-edge agent reasoning approaches, moving
beyond conventional methods to explore novel paradigms emerging from recent research. Drawing
extensively from foundational concepts such as Atom of Thoughts (AoT), AlphaEvolve, and the Darwin
Gödel Machine (DGM), this analysis identifies a pivotal shift towards meta-level self-optimization,
adaptive strategy selection, and autonomous AI design. The discussion delves into the intricate
mechanisms of these frameworks, their demonstrated performance benefits, and their broader
implications for the future development and deployment of intelligent agents. By synthesizing these
diverse advancements, the report illuminates a trajectory where AI systems not only perform complex
tasks with enhanced efficiency but also continuously learn to refine their own cognitive architectures
and problem-solving methodologies.
1. Introduction
1.1 The Evolving Landscape of LLM Agent Reasoning
Large Language Models (LLMs) have achieved remarkable strides in complex reasoning tasks, propelled
by the adoption of increasingly sophisticated approaches. This evolution signifies a significant departure
from rudimentary text generation, moving towards the emulation of more intricate cognitive processes.
Initially, advancements such as Chain-of-Thought (CoT) prompting enabled LLMs to articulate
intermediate reasoning steps, a mechanism that closely mirrors human problem-solving methodologies.
These early methods were designed to simulate a form of "fast thinking," providing direct, step-by-step
derivations for problem resolution.
However, the field has progressed to enable what is termed "slower, more deliberate thinking" in LLMs.
This involves mechanisms for iterative refinement, reflection, and tree search, which are crucial for
navigating subsequent reasoning steps. This progression has substantially elevated LLM performance
and simultaneously brought heightened attention to the trustworthiness and robustness of these
complex reasoning processes. The inherent dependence on step-wise reasoning introduces a particular
susceptibility: manipulation of initial reasoning steps can propagate errors, leading to cascading failures
throughout the entire reasoning chain.
The progression of LLM reasoning, from straightforward prompting techniques like Chain-of-Thought to
more intricate iterative and tree-based methodologies such as Tree-of-Thought (ToT) and Graph-of-
Thought (GoT), indicates a fundamental reorientation in AI development. This is not merely about
improving the output of reasoning by making derivations explicit, but rather about optimizing the
process of reasoning itself. The shift from linear, "fast thinking" to branching, "slow thinking" is a direct
response to the fragility and lack of resilience observed in simpler reasoning chains. The objective
extends beyond achieving higher accuracy; it encompasses building AI systems whose decision-making is
transparent, verifiable, and robust against errors. By empowering agents to reflect, self-critique, and
explore alternative solution paths, the aim is to create systems that can re-evaluate and correct their
course, much like human cognition. This emphasis on process-level integrity is becoming increasingly
vital for the reliable deployment of AI in real-world, high-stakes applications.
The broader field of AI, particularly the domain of agentic systems, is undergoing rapid development,
with no expectation of reaching a static or stable state in the foreseeable future. This accelerating pace
Novel Agent Reasoning Approaches DeepResearch Gemini
Wednesday, June 4, 2025 6:32 AM
Programming Page 28
with no expectation of reaching a static or stable state in the foreseeable future. This accelerating pace
is further underscored by significant market interest, with the agentic AI sector valued at USD 5.2 billion
in late 2024 and projected to expand to nearly USD 200 billion by 2034.
1.2 Purpose and Scope of this Report: Unveiling Novel Paradigms
This report is designed to provide an expert-level analysis of cutting-edge agent reasoning approaches
that are currently emerging from the forefront of research. The focus is on methods that have not yet
been broadly integrated into mainstream discussions, highlighting their novelty and potential impact.
The analysis will leverage the foundational concepts and references from three pivotal papers: "Atom of
Thoughts" (AoT), "AlphaEvolve," and the "Darwin Gödel Machine" (DGM). These works will serve as a
framework to contextualize and uncover other novel methodologies. The scope of this report
encompasses a deep technical examination of the underlying mechanisms of these approaches, their
reported performance benefits, and a critical analysis of their potential synergies and broader
implications for the future trajectory of AI development.
2. Foundational Pillars of Advanced Agent Reasoning
This section explores three seminal works that represent significant advancements in agent reasoning
and self-improvement: Atom of Thoughts, AlphaEvolve, and the Darwin Gödel Machine. Each introduces
a distinct yet complementary paradigm, collectively pointing towards a future of more autonomous and
capable AI.
2.1 Atom of Thoughts (AoT): Markovian Decomposition for Efficient
Reasoning
Atom of Thoughts (AoT) is a novel framework that introduces a Markov-style reasoning process for
Large Language Models (LLMs). Its primary objective is to address the inefficiencies stemming from the
accumulation of historical information in existing test-time scaling methods, which often consumes
excessive computational resources and impedes effective reasoning.
The core observation underpinning AoT is that complex reasoning can be broken down into a series of
independent and self-contained "atomic questions". These atomic questions exhibit a memoryless
property, akin to Markov processes, where each reasoning state is dependent solely on the current
state. This approach mirrors human problem-solving, where individuals naturally identify and resolve
self-evident sub-problems first, then seamlessly integrate these solutions to reformulate a simplified
problem state, rather than retaining the detailed reasoning processes for resolved components.
The framework operates through an iterative two-phase state transition mechanism:
Decomposition: The initial complex question is first broken down into a dependency-based
Directed Acyclic Graph (DAG). This phase is crucial for capturing the structural dependencies
between sub-problems, laying the groundwork for subsequent simplification.
1.
Contraction: Following decomposition, the identified subquestions within the DAG are
"contracted" or resolved, leading to a new, simplified atomic question state. This iterative
decomposition-contraction process continues until the problem is reduced to a set of directly
solvable atomic questions.
2.
This design provides two significant advantages. First, AoT substantially reduces computational resource
consumption by eliminating the need to maintain and process redundant historical information. Second,
AoT can function either as a standalone reasoning framework or as a plug-in enhancement for existing
test-time scaling methods, thereby improving both their performance and cost efficiency. Empirical
evaluations across six benchmarks demonstrate its effectiveness, notably enabling GPT-4o-mini to
surpass larger reasoning models like o3-mini and DeepSeek-R1 on the HotpotQA dataset.
2.2 AlphaEvolve: Evolutionary Algorithm for Algorithmic and Scientific
Discovery
AlphaEvolve, a groundbreaking coding agent developed by Google's DeepMind, orchestrates an
autonomous pipeline of LLMs to iteratively improve algorithms by directly modifying their code. This
system employs an evolutionary approach, continuously receiving feedback from evaluators to refine
the algorithms it generates.
The operational components of AlphaEvolve include:
Programming Page 29
The operational components of AlphaEvolve include:
Fitness Function: A clearly defined, measurable fitness function is essential to quantify success
and guide the evolutionary process towards optimal solutions. This ensures that the system's
improvements are empirically verifiable.
•
Smart Prompt Generation: The LLM's internal context dynamically adapts with each inference.
This adaptation is informed by both successful and unsuccessful past code attempts, along with
their corresponding fitness results. This mechanism allows the LLM to learn from its prior
"experience" and generate more effective prompts for subsequent code modifications.
•
Evolutionary Algorithm: AlphaEvolve utilizes a sophisticated evolutionary algorithm that
integrates MAP-Elites with island-based population models. This architectural choice enables
subpopulations to evolve independently, fostering diversity through mutations and effectively
preventing premature convergence to local optima within the vast search space.
•
Dual LLM Architecture: The system employs two distinct base LLMs: a primary model optimized
for rapid idea generation, and a stronger secondary LLM dedicated to enhancing the quality of the
generated code. While the algorithm's overall effectiveness is independent of the specific LLM
models used, more powerful models consistently yield superior results.
•
AlphaEvolve is engineered to facilitate novel research by refining code until it solves problems in a highly
optimized manner. Its applicability spans not only problems where the discovery of new algorithms is
the intrinsic goal but also broader computational challenges where an algorithm defines how a solution
is constructed. The system significantly surpasses its predecessors in terms of scale and generality,
capable of evolving large, complex pieces of code that incorporate multiple functions and components.
Notable achievements include the discovery of slightly faster algorithms for matrix multiplication,
improving upon the state-of-the-art for 14 algorithms, including a novel 4x4 complex-valued matrix
multiplication algorithm using only 48 multiplications. It has also been successfully applied to finding
search algorithms for various mathematical problems, improving data center scheduling algorithms,
simplifying hardware accelerator circuit designs, and even accelerating the training of the LLM
underpinning AlphaEvolve itself. Across over 50 computational problems, AlphaEvolve has surpassed
state-of-the-art solutions and discovered new, provably better constructions in approximately 20% of
cases.
2.3 Darwin Gödel Machine (DGM): Empirical Self-Modification for
Open-Ended AI
The Darwin Gödel Machine (DGM) represents a significant advancement towards self-improving AI. It is
a system that iteratively modifies its own code, crucially including the code responsible for its ability to
modify its own codebase. This capability positions DGM as a substantial step towards achieving open-
ended innovation in AI.
DGM offers a pragmatic alternative to the theoretical Gödel Machine, drawing inspiration from
Darwinian evolution and open-endedness research. The original Gödel Machine concept proposed self-
modifications only when the AI could mathematically prove a better strategy, an assumption that has
proven impractical to realize. DGM, in contrast, embraces empirical validation, testing each proposed
code change using coding benchmarks to assess its benefit. This shift acknowledges that for highly
complex, emergent AI behaviors, achieving formal mathematical proofs of optimality or safety is often
intractable. The DGM's approach demonstrates that progress in self-improving AI can be made by
adopting an iterative, empirically driven methodology.
The workflow of DGM operates through several key stages:
Initialization: The evolutionary process commences with one or a few basic "seed" coding agents,
which may possess only minimal functionalities. DGM maintains an "archive of agents,"
functioning as a "biological gene bank" that meticulously preserves all previously generated
agents, thereby ensuring that no potentially valuable variation is permanently lost.
1.
Sampling: DGM selects one or more parent agents from this archive to generate new child agents.
The selection mechanism is designed to encourage exploration, granting "less successful agents"
an opportunity to contribute to future generations, even as higher-performing agents are more
likely to be chosen.
2.
Reproduction: Foundation models are utilized to propose modifications to the selected parent3.
Programming Page 30
Reproduction: Foundation models are utilized to propose modifications to the selected parent
agents' source code. These changes can be comprehensive, encompassing enhancements to
existing tools (e.g., upgrading a file-editing tool to perform precise string replacement), the
addition of entirely new tools or workflows (e.g., integrating a static code analysis tool or
designing a "think-act-observe" loop), improvements to problem-solving strategies (e.g., refining
how the agent prompts the base model or managing long-context code more effectively), and the
introduction of collaboration mechanisms (e.g., having an agent's code reviewed by another
before execution).
3.
Natural Selection: Newly generated child agents are not automatically integrated but must prove
their worth through rigorous testing. Their performance, such as the percentage of successfully
solved problems, is quantitatively scored, guiding the selection of the most effective agents.
4.
Tree Formation: If a child agent outperforms its parent or meets predefined quality thresholds, it
is added to the archive, becoming a new node in a continuously growing evolutionary tree. This
process facilitates the parallel exploration of many different paths within the search space,
promoting open-ended discovery and preventing premature convergence to suboptimal local
solutions.
5.
DGM automatically improves its coding capabilities, demonstrating significant performance increases on
coding benchmarks. For instance, performance on SWE-bench improved from 20.0% to 50.0%, and on
Polyglot from 14.2% to 30.7%. It consistently outperforms baselines that lack self-improvement or open-
ended exploration, underscoring the critical role of self-modification in its success. All experiments are
conducted with stringent safety precautions, including sandboxing and human oversight, to manage the
inherent risks of autonomous self-improvement. This reliance on robust empirical evaluation,
continuous testing, and integrated safety mechanisms reflects a growing practical approach to managing
the uncertainties associated with autonomous self-improvement, rather than waiting for unattainable
theoretical guarantees.
The convergence of "thoughts" (as exemplified by AoT's Markovian decomposition for efficient
inference-time processing) with "evolution" (as demonstrated by AlphaEvolve and DGM's continuous
self-improvement of agent capabilities and code) signifies a paradigm shift towards meta-level self-
optimization in AI. AoT focuses on optimizing the execution of reasoning for a given task, making the
process more efficient and resource-conscious. In parallel, AlphaEvolve and DGM are fundamentally
concerned with self-improvement, operating at a meta-level to enhance the agent's ability to perform
tasks or even to evolve its own architectural design principles. This synergy suggests a future where AI
systems can not only perform complex computations efficiently but also autonomously discover, design,
and refine optimal problem-solving architectures and strategies for themselves. Imagine a DGM-like
system that, through its evolutionary loop, learns to implement an AoT-like decomposition strategy for
new, complex problems, or an AlphaEvolve that discovers novel atomic operators that can be seamlessly
integrated into AoT's framework. This combination points to a truly open-ended intelligence that
continuously adapts and improves its core cognitive functions.
Table 1: Comparison of Foundational Agent Reasoning Paradigms
Paradigm Core
Mechanism
Primary Focus Key
Advantage/Benefi
t
Self-Improvement
Strategy
Reported
Performance/Im
pact
Atom of
Thoughts
(AoT)
Markovian
decompositio
n into atomic
questions
Test-time
computational
efficiency and
effective
reasoning
Reduces
computational
resources by
eliminating
historical
information
overhead
Iterative
decomposition-
contraction of
problem states
GPT-4o-mini
surpasses larger
models (o3-mini,
DeepSeek-R1) on
HotpotQA
AlphaEvol
ve
LLM-
orchestrated
evolutionary
search for
Algorithmic
and scientific
discovery
through code
Discovers novel,
provably better
algorithms and
optimizes
Evolutionary
refinement of LLM
context and
generated code
Improves SOTA
for 14 matrix
multiplication
algorithms,
Programming Page 31
search for
code
optimization
through code
evolution
optimizes
computational
infrastructure
generated code
via fitness
functions and
mutations
algorithms,
optimizes data
center scheduling
Darwin
Gödel
Machine
(DGM)
Iterative
empirical self-
modification
of agent code
Open-ended
autonomous
self-
improvement
of AI
capabilities
Automatically
improves coding
capabilities and
outperforms
baselines without
self-improvement
Empirical
validation of code
changes through
coding
benchmarks and
an archive of
agents
Increases
performance on
SWE-bench
(20.0% to 50.0%)
and Polyglot
(14.2% to 30.7%)
3. Emerging and Novel Agent Reasoning Approaches
Beyond the foundational paradigms discussed, a new wave of agent reasoning approaches is emerging,
characterized by adaptive strategies, advanced self-improvement mechanisms, and fine-grained control
over operations.
3.1 Adaptive and Dynamic Reasoning Frameworks
The development of adaptive and dynamic reasoning frameworks marks a significant progression in LLM
capabilities, moving beyond static, one-size-fits-all approaches.
Derailer-Rerailer This framework directly addresses the critical trade-off between reasoning accuracy
and computational efficiency in LLMs. It introduces a lightweight "Derailer" mechanism designed to
assess the stability of the LLM's reasoning process. Crucially, an advanced "Rerailer" verification process
is triggered selectively, only when reasoning instability is detected. This adaptive approach optimizes
computational resource usage by avoiding the indiscriminate deployment of computationally expensive
procedures across all queries, a common limitation in prior complex prompting methods. Derailer-
Rerailer achieves significant accuracy improvements, ranging from 8% to 11% across various reasoning
tasks, while maintaining a 2 to 3 times better computational efficiency. Its novel contribution lies in this
adaptive verification mechanism, which dynamically balances resource allocation with reliability, making
it particularly suitable for real-time, latency-sensitive LLM applications such as clinical support.
RL-of-Thoughts (RLoT) RLoT represents an innovative approach that trains a lightweight "navigator
model" using reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. This
framework aims to overcome the limitations of manually predefined, task-agnostic reasoning
frameworks that often lack the necessary adaptability. The core concept involves designing five
fundamental "logic blocks" inspired by human cognitive processes. During the reasoning process, the
trained RL navigator dynamically selects the most appropriate logic blocks and combines them into task-
specific logical structures, tailored to the unique characteristics of the problem at hand. RLoT empirically
outperforms established inference-time techniques by up to 13.4% across multiple reasoning
benchmarks, including AIME, MATH, and GPQA, and demonstrates effectiveness with various LLMs such
as GPT, Llama, Qwen, and DeepSeek. Remarkably, its compact RL navigator, with fewer than 3,000
parameters, enables sub-10B LLMs to achieve performance comparable to 100B-scale counterparts.
Furthermore, RLoT exhibits strong transferability, generalizing effectively to unseen LLMs and tasks. Its
adaptive, RL-driven selection of reasoning components at inference time constitutes a novel paradigm
for flexible and efficient LLM reasoning.
Diagram of Thought (DoT) DoT is a framework that models iterative reasoning within a single Large
Language Model as the progressive construction of a Directed Acyclic Graph (DAG). Unlike linear chains
or simple tree structures, DoT organizes propositions, critiques, refinements, and verifications into a
cohesive DAG, enabling the exploration of complex, non-linear reasoning pathways while rigorously
maintaining logical consistency. Each node within the DAG represents a proposition at various stages of
evaluation. DoT incorporates natural language critiques, which provide richer and more informative
feedback than traditional binary signals. This detailed feedback facilitates a deeper understanding and
more effective refinement of propositions by the LLM itself. DoT distinguishes itself by unifying the
strengths of prior approaches, such as Chain-of-Thought and Tree-of-Thought, within a single, self-
Programming Page 32
strengths of prior approaches, such as Chain-of-Thought and Tree-of-Thought, within a single, self-
contained LLM. It leverages auto-regressive next-token prediction, augmented with special, role-specific
tokens (e.g., <proposer>, <critic>, <summarizer>), to internally manage role transitions and reasoning
steps. This streamlines the reasoning process and simplifies implementation by eliminating the need for
multi-LLM collaboration or external control mechanisms. The framework is also formalized using Topos
Theory, providing a robust mathematical foundation that ensures logical consistency and soundness
throughout the reasoning process.
LookPlanGraph LookPlanGraph is an embodied instruction following method specifically designed for
autonomous agents operating in dynamic environments. It leverages hierarchical scene graphs but,
critically, dynamically augments these graphs during task execution using a Visual Language Model
(VLM) and the agent's egocentric camera. The approach begins by initializing its scene graph with only
immobile static objects. As the agent interacts with its surroundings, the VLM processes real-time
images from the agent's camera to identify and integrate movable objects, along with their states and
relationships, into the evolving scene graph. It employs a Memory Graph Mechanism to adapt to
environmental changes by focusing on relevant, nearby objects, and a Graph Augmentation Mechanism
for real-time exploration and updates. LookPlanGraph effectively handles tasks in dynamic
environments, demonstrating superior performance compared to approaches that rely solely on pre-
created static scene graphs. It addresses significant limitations such as the inability to perceive hidden
objects (by enabling "look inside" actions) and the computational burden of large closed models by
aiming for effective planning with smaller LLMs. Its novel contribution lies in the dynamic, VLM-
augmented scene graph, which provides real-time environmental grounding and adaptation capabilities
for embodied agents.
A notable development observed across these frameworks is a clear progression towards adaptive and
dynamic resource allocation and strategy selection in LLM reasoning. Instead of universally applying
computationally intensive methods, frameworks like Derailer-Rerailer and RLoT demonstrate that an
intelligent, context-aware selection of reasoning strategies significantly improves both efficiency and
overall performance. This mirrors human expert behavior, where the allocation of cognitive effort is
proportional to the perceived difficulty or novelty of a problem. This development suggests a move
towards more "cognitively economical" AI, where computational thought is allocated strategically rather
than uniformly. Future agent architectures will likely feature sophisticated "control plane" mechanisms
that dynamically orchestrate and optimize the use of underlying reasoning components, leading to more
practical and scalable deployments in complex real-world scenarios.
3.2 Advanced Self-Improvement and Workflow Automation
The proliferation of self-improvement mechanisms across diverse domains underscores a foundational
shift in AI development.
UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents UI-Genie
is a self-improving framework specifically designed for Multimodal Large Language Model (MLLM)-based
Mobile GUI Agents. It tackles two critical challenges: the difficulty of verifying trajectory outcomes and
the scalability of acquiring high-quality training data. The system incorporates a specialized reward
model, UI-Genie-RM, featuring an image-text interleaved architecture that efficiently processes
historical context and unifies both action-level and task-level rewards. To address data scarcity, UI-Genie
employs deliberate data generation strategies, including rule-based verification, controlled trajectory
corruption, and hard negative mining. A self-improvement pipeline progressively expands the range of
solvable complex GUI tasks by iteratively enhancing both the agent and its reward models through
reward-guided exploration and outcome verification in dynamic environments. UI-Genie has achieved
state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-
model self-improvement. It successfully generates high-quality synthetic trajectories without requiring
manual annotation, establishing the first reward-specific dataset for GUI agents (UI-Genie-RM-517k).
The iterative co-evolution of the agent and its reward model, driven by reward-guided exploration and
outcome verification, represents a novel contribution to autonomous GUI agent training.
LLM-Guided Evolution (LLM-GE) & Evolution of Thought (EoT): LLMs Optimizing Model Architectures
LLM-Guided Evolution (LLM-GE) is a novel framework that integrates the human-like expertise of LLMs
with Neural Architecture Search (NAS) through genetic algorithms. Unlike traditional NAS methods that
rely on fixed rules and predefined building blocks, LLM-GE leverages LLMs to directly modify model
Programming Page 33
rely on fixed rules and predefined building blocks, LLM-GE leverages LLMs to directly modify model
source code, such as YAML configuration files for YOLO models, and intelligently guide the processes of
mutations and crossovers. Central to LLM-GE is the "Evolution of Thought" (EoT) technique. EoT
establishes feedback loops that enable LLMs to iteratively refine their decisions based on the empirical
performance of their prior code augmentations. This mechanism catalyzes LLMs to introspect and fine-
tune suggestions, creating a self-enhancing feedback loop for architectural evolution. LLM-GE has
successfully produced variants of YOLO models with significant performance improvements in object
detection, such as an increase in Mean Average Precision from 92.5% to 94.5%. It maintains genetic
diversity, which is crucial for evolutionary algorithms, while injecting expert-like creativity and insight
into the process. The direct LLM-driven modification of model source code, combined with result-driven
feedback loops (EoT) for autonomous model architecture optimization, represents a novel paradigm for
automated machine learning.
EvoFlow: Evolving Diverse Agentic Workflows On The Fly EvoFlow is a niching evolutionary algorithm-
based framework designed to automatically search for and generate a population of heterogeneous and
complexity-adaptive agentic workflows. It addresses the limitations of existing automated pipelines that
often lack LLM heterogeneity and focus only on single-objective performance optimization. EvoFlow
innovatively frames agentic search as a multi-objective optimization problem, considering both cost and
performance to generate a Pareto-optimal set of workflows. It utilizes "operator nodes," which are LLM-
agent invoking nodes, as its fundamental units. The framework continuously evolves workflows by
performing tag-based retrieval of parent workflows, applying crossover to generate offspring, and
introducing extensive mutation functions (including LLM, prompt, and operator mutations). Niching-
based selection is employed to maintain population diversity and quality. EvoFlow has demonstrated its
diversity by evolving workflows ranging from simple I/O tasks to complex multi-turn interactions. It is
high-performing, surpassing previous handcrafted and automated workflows by 1.23% to 29.86%.
Crucially, it is economical, outperforming powerful proprietary models like o1-preview at a fraction of
the inference cost, by leveraging weaker open-source models. Its explicit formulation of agentic
workflow automation as a cost-performance multi-objective optimization problem and its niching
evolutionary algorithm for autonomous, diverse workflow evolution are key novelties.
AFlow: Automating Agentic Workflow Generation AFlow is an automated framework that efficiently
explores the vast search space of agentic workflows using Monte Carlo Tree Search (MCTS). It iteratively
refines workflows through code modification, tree-structured experience, and execution feedback.
Workflows are modeled as interconnected LLM-invoking nodes with code-based edges, providing
precise control over execution flow. The MCTS process in AFlow includes a soft mixed-probability
selection mechanism for node exploration, LLM-driven expansion to introduce new possibilities, direct
execution evaluation to assess workflow performance, and backpropagation of experience to refine
future search. It also introduces "Operators"—predefined, reusable combinations of nodes representing
common agentic operations—as foundational building blocks to enhance search efficiency. AFlow yields
a 5.7% average improvement over state-of-the-art baselines. Notably, workflows generated by AFlow
enable smaller LLMs to outperform larger models like GPT-4o on specific tasks at a significantly lower
inference cost (e.g., 4.55%). The MCTS-based framework for automating agentic workflow generation,
particularly with its code-represented edges and "Operators" for structured exploration, is a novel
contribution to achieving fully automated and effective workflow design.
RL-enhanced Evolutionary Search for Algorithm Discovery This novel approach augments traditional
LLM-based evolutionary search by continuously refining the "search operator"—the LLM itself—through
reinforcement learning (RL) fine-tuning. It moves beyond treating the LLM as a static generator in
evolutionary processes. The method leverages evolutionary search as an exploration strategy to
discover improved algorithms, while RL optimizes the LLM's policy based on the feedback (evaluation
scores) obtained from these discoveries. This synergy aligns with the "Bitter Lesson" principle, where
search generates new data, and learning distills patterns to guide future exploration more effectively.
The LLM is trained in-weight using the evaluation scores of generated programs as the reward signal.
Experiments on combinatorial optimization tasks, including bin packing, traveling salesman, and the
flatpack problem, demonstrate that combining RL and evolutionary search improves the efficiency of
discovering improved algorithms. The novelty lies in the direct integration of RL fine-tuning on the LLM
(as the active search operator) within an evolutionary search loop, enabling the LLM to continuously
Programming Page 34
(as the active search operator) within an evolutionary search loop, enabling the LLM to continuously
learn and adapt its algorithm generation capabilities based on empirical performance, leading to more
effective algorithm discovery.
The proliferation of self-improvement mechanisms across diverse domains, from GUI agents to model
architecture and workflow generation, indicates a foundational shift from human-designed AI systems to
AI systems that autonomously design, optimize, and improve themselves. This progression suggests that
the primary bottleneck in AI advancement may transition from human ingenuity in devising new models
and algorithms to the availability of robust, automated evaluation environments and scalable, safe self-
improvement loops that can effectively guide these autonomous systems. The ability of AI to self-modify
and self-optimize its own capabilities and even its underlying architecture opens up unprecedented
avenues for progress, but also necessitates rigorous attention to the development of reliable validation
and safety protocols.
3.3 Fine-Grained Control and Atomic Operations
Achieving higher levels of autonomy and performance in AI agents necessitates increasingly fine-grained
control over their internal operations and the ability to compose complex behaviors from atomic units.
Policy Optimization with Action Decomposition (POAD): Token-Level Optimization for LLM Agents
POAD proposes decomposing language agent optimization from the action level down to the token
level, providing finer supervision for each intra-action token. This approach directly addresses the
challenges of limited environmental dynamics knowledge and exponentially large action spaces that LLM
agents often encounter. The core innovation is the derivation of the Bellman backup with Action
Decomposition (BAD). BAD integrates credit assignments for both intra-action tokens (tokens within a
single action) and inter-action tokens (tokens across different actions), effectively eliminating
discrepancies between traditional action-level optimization and naive token-level optimization. This
ensures theoretical consistency with optimizing the original Markov Decision Process (MDP). POAD
implements BAD within the Proximal Policy Optimization (PPO) algorithm. POAD benefits from a finer-
grained credit assignment process and lower optimization complexity, transforming the optimization
problem from an intractable O(|V|^|a|) action space to a more manageable O(|a| × |V|) token space.
This leads to enhanced learning efficiency and generalization abilities in aligning language agents with
interactive environments. The theoretical soundness of BAD in eliminating the "later tokens are more
important" assumption for linguistic actions is a key novelty.
Offline REasoning Optimization (OREO): Offline RL with Token-Level Value Functions for Multi-Step
Reasoning OREO is an offline reinforcement learning (RL) method specifically designed to enhance the
multi-step reasoning abilities of LLMs. It addresses limitations of other offline RL methods, particularly
Direct Preference Optimization (DPO), which relies on paired preference data and treats all tokens
uniformly, rendering it less suitable for multi-step reasoning tasks with sparse rewards. Building on
maximum entropy RL principles, OREO jointly learns a policy model and a value function by optimizing
the soft Bellman Equation. A key feature is its use of a token-level value function, which enables finer-
grained credit assignment—a crucial aspect for multi-step reasoning where correctness often hinges on
a few key tokens. It can effectively leverage unpaired data, even when only sparse rewards are available.
OREO consistently surpasses existing offline learning methods, such as rejection sampling, DPO, and
KTO, on various multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K,
MATH) and embodied agent control (ALFWorld). The learned value function can also be leveraged to
guide tree search at test time, further boosting performance. Its ability to utilize unpaired data and its
token-level value function for fine-grained credit assignment in multi-step reasoning, combined with its
iterative framework and value-guided test-time search, contribute to its novelty.
AtomR: Atomic Operator-Empowered LLMs for Heterogeneous Knowledge Reasoning AtomR is a
framework that empowers LLMs to conduct accurate heterogeneous knowledge reasoning at an "atomic
level". It draws inspiration from how knowledge graphs explicitly model compositional reasoning
through the combination of atomic components. AtomR proposes three fundamental atomic knowledge
operators: Search, Relate, and Filter. These operators possess properties of indivisibility and
orthogonality, meaning each corresponds to a distinct knowledge operation without functional overlap.
By composing these atomic operators, complex procedures of knowledge-intensive reasoning can be
effectively modeled. The framework operates through a two-stage pipeline:
Atomic Reasoning Planning: AtomR decomposes a complex input question into a hierarchical1.
Programming Page 35
Atomic Reasoning Planning: AtomR decomposes a complex input question into a hierarchical
Atomic Reasoning Tree (ART). Each leaf node in the ART corresponds to one of the three
predefined atomic knowledge operators, ensuring highly fine-grained and orthogonal question
decomposition.
1.
Atomic Reasoning Execution: The framework answers the original question by recursively
executing the reasoning tree in a bottom-up order. Leaf atomic operators dynamically select,
retrieve, and manipulate knowledge from diverse heterogeneous sources, including local text
corpora, online web pages, and structured knowledge graphs. Non-leaf nodes either utilize "Child
Answer Reasoning" or, if that fails, trigger "Direct RAG Reasoning," which involves retrieval for the
current node, thereby enhancing robustness and cost efficiency.
2.
AtomR significantly outperforms state-of-the-art baselines on heterogeneous knowledge reasoning,
demonstrating F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on BlendQA. Its explicit
definition and composition of atomic knowledge operators for fine-grained, heterogeneous knowledge
reasoning, combined with the hierarchical ART planning, represents a novel approach to enhancing LLM
reasoning and minimizing issues such as hallucination.
4. Conclusions
The examination of cutting-edge agent reasoning approaches reveals a dynamic and rapidly evolving
landscape in artificial intelligence. The progression from foundational paradigms like Atom of Thoughts,
AlphaEvolve, and the Darwin Gödel Machine to more specialized frameworks highlights several critical
trends shaping the future of AI agents.
First, a profound shift is observed towards meta-level self-optimization. Atom of Thoughts optimizes the
execution of reasoning for efficiency, while AlphaEvolve and the Darwin Gödel Machine focus on
evolving the agent itself or its design principles. This convergence points to a future where AI systems
not only solve problems but also continuously learn to discover, design, and refine their own problem-
solving architectures and strategies. This capability to autonomously improve core cognitive functions
marks a significant leap towards truly open-ended intelligence.
Second, there is a clear and growing emphasis on adaptive and dynamic resource allocation and strategy
selection. Frameworks like Derailer-Rerailer and RL-of-Thoughts exemplify this by intelligently
determining when and how much computational effort to expend, and which reasoning strategy to
employ, based on the problem's characteristics or the perceived stability of the current reasoning path.
This mirrors human expert behavior, where cognitive resources are deployed strategically rather than
uniformly. Such approaches are crucial for the practical and scalable deployment of AI in real-world
scenarios, where efficiency and reliability are paramount.
Finally, the widespread adoption of self-improvement mechanisms across diverse domains, from
optimizing mobile GUI agents (UI-Genie) and model architectures (LLM-Guided Evolution) to automating
workflow generation (EvoFlow, AFlow) and algorithm discovery (RL-enhanced Evolutionary Search),
signifies a foundational transition. AI systems are moving from being human-designed artifacts to
becoming autonomous entities capable of designing, optimizing, and improving themselves. This
suggests that the future bottleneck in AI progress may not be human ingenuity in devising new models,
but rather the availability of robust, automated evaluation environments and scalable, safe self-
improvement loops that can effectively guide these self-evolving systems. The pragmatic shift from
requiring provable benefits to embracing empirically validated self-modifications, coupled with
integrated safety mechanisms, underscores the engineering-centric approach necessary for navigating
the complexities of autonomous AI development.
In summary, the next generation of AI agents will be characterized by their ability to not only reason
effectively but also to continuously learn, adapt, and self-improve their very methods of reasoning and
operation. This trajectory promises increasingly capable and autonomous AI, while simultaneously
necessitating rigorous attention to the development of robust validation, safety, and oversight protocols
to ensure beneficial outcomes.
Sources used in the report
Programming Page 36
arxiv.org
Stepwise Reasoning Disruption Attack of LLMs - arXiv
Opens in a new window
arxiv.org
Evaluating Mathematical Reasoning Across Large Language Models: A Fine-Grained Approach - arXiv
Opens in a new window
richardcsuwandi.github.io
AI that can improve itself - Richard Cornelius Suwandi
Opens in a new window
towardsdatascience.com
Google's AlphaEvolve: Getting Started with Evolutionary Coding Agents
Opens in a new window
arxiv.org
Small Language Models are the Future of Agentic AI - arXiv
Opens in a new window
arxiv.org
Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research -
arXiv
Opens in a new window
openreview.net
ICLR 2025 Workshop LLM Reason and Plan | OpenReview
Opens in a new window
proceedings.neurips.cc
Reinforcing LLM Agents via Policy Optimization with Action ...
Opens in a new window
storage.googleapis.com
storage.googleapis.com
Opens in a new window
Programming Page 37
openreview.net
LookPlanGraph: Embodied instruction following method with VLM graph augmentation
Opens in a new window
arxiv.org
UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents - arXiv
Opens in a new window
arxiv.org
Atom of Thoughts for Markov LLM Test-Time Scaling - arXiv
Opens in a new window
arxiv.org
[2505.21496] UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI
Agents - arXiv
Opens in a new window
arxiv.org
Atom of Thoughts for Markov LLM Test-Time Scaling - arXiv
Opens in a new window
arxiv.org
[2412.16145] Offline Reinforcement Learning for LLM Multi-Step Reasoning - arXiv
Opens in a new window
arxiv.org
Reinforcing Language Agents via Policy Optimization with Action Decomposition - arXiv
Opens in a new window
arxiv.org
Derailer-Rerailer: Adaptive Verification for Efficient and Reliable Language Model Reasoning - arXiv
Opens in a new window
openreview.net
openreview.net
Opens in a new window
Programming Page 38
proceedings.neurips.cc
proceedings.neurips.cc
Opens in a new window
openreview.net
openreview.net
Opens in a new window
arxiv.org
Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning - arXiv
Opens in a new window
arxiv.org
[2505.14140] RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning -
arXiv
Opens in a new window
arxiv.org
AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge
Reasoning - arXiv
Opens in a new window
huggingface.co
Paper page - Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents
Opens in a new window
arxiv.org
Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents - arXiv
Opens in a new window
arxiv.org
LLM Guided Evolution - The Automation of Models Advancing Models - arXiv
Opens in a new window
arxiv.org
Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning - arXiv
Opens in a new window
Programming Page 39
arxiv.org
[2502.12018] Atom of Thoughts for Markov LLM Test-Time Scaling - arXiv
Opens in a new window
researchgate.net
Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning
Opens in a new window
scribd.com
Atom of Thoughts for Markov LLM Test-Time Scaling-2025 - Scribd
Opens in a new window
arxiv.org
AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge
Reasoning - arXiv
Opens in a new window
arxiv.org
On the Diagram of Thought - arXiv
Opens in a new window
arxiv.org
On the Diagram of Thought - arXiv
Opens in a new window
arxiv.org
[2410.10762] AFlow: Automating Agentic Workflow Generation - arXiv
Opens in a new window
arxiv.org
EvoFlow: Evolving Diverse Agentic Workflows On The Fly - arXiv
Opens in a new window
arxiv.org
[2502.07373] EvoFlow: Evolving Diverse Agentic Workflows On The Fly - arXiv
Opens in a new window
Programming Page 40
arxiv.org
arxiv.org
Opens in a new window
arxiv.org
arxiv.org
Opens in a new window
arxiv.org
arXiv:2502.12018v2 [cs.CL] 23 Mar 2025
Opens in a new window
Sources read but not used in the report
arxiv.org
Comparative Analysis of AI Agent Architectures for Entity Relationship Classification - arXiv
Opens in a new window
arxiv.org
A Survey of AI Agent Protocols - arXiv
Opens in a new window
machinelearning.apple.com
Apple Machine Learning Research at ICLR 2025
Opens in a new window
proceedings.neurips.cc
Aligning LLM Agents by Learning Latent Preference from User Edits
Opens in a new window
2025.emnlp.org
New Tracks at EMNLP 2025 and Their Relationship to ARR Tracks
Opens in a new window
2025.emnlp.org
The 2025 Conference on Empirical Methods in Natural Language Processing - EMNLP 2025
Opens in a new window
Programming Page 41
2025.aclweb.org
Accepted Industry Track Papers - ACL 2025
Opens in a new window
2025.aclweb.org
Accepted Findings Papers - ACL 2025
Opens in a new window
arxiv.org
A Survey of Scaling in Large Language Model Reasoning - arXiv
Opens in a new window
arxiv.org
arXiv:2503.16416v1 [cs.AI] 20 Mar 2025
Opens in a new window
arxiv.org
[2505.14652] General-Reasoner: Advancing LLM Reasoning Across All Domains - arXiv
Opens in a new window
arxiv.org
Group-in-Group Policy Optimization for LLM Agent Training - arXiv
Opens in a new window
arxiv.org
ProgRM: Build Better GUI Agents with Progress Rewards - arXiv
Opens in a new window
arxiv.org
Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks - arXiv
Opens in a new window
arxiv.org
LEAP: LLM-powered End-to-end Automatic Library for Processing Social Science Queries on Unstructured
Data - arXiv
Opens in a new window
Programming Page 42
arxiv.org
ProgRM: Build Better GUI Agents with Progress Rewards - arXiv
Opens in a new window
arxiv.org
Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning - arXiv
Opens in a new window
arxiv.org
[2504.07128] DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning - arXiv
Opens in a new window
arxiv.org
Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments -
arXiv
Opens in a new window
arxiv.org
Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-
Language Models - arXiv
Opens in a new window
arxiv.org
arXiv:2503.22402v1 [cs.DB] 28 Mar 2025
Opens in a new window
selfawaresystems.com
The Nature of Self-Improving Artificial Intelligence
Opens in a new window
arxiv.org
promptbreeder: self-referential self-improvement - arXiv
Programming Page 43
https://medium.com/ai-simplified-in-plain-english/deepseek-r1s-cot-revolution-rethinking-ai-reasoning-
from-chains-to-hypergraphs-2baecbb5fe26
DeepSeek R1’s CoT Revolution: Rethinking AI Reasoning
from Chains to Hypergraphs
Discover the evolution from linear CoT to Tree-, Graph-, and
Program-of-Thoughts — and what it means for human-AI
synergy
JIN
7 min read
·
Apr 26, 2025
Disclosure: I use GPT search to collect facts. The entire article is drafted by me.
Introduction
As artificial intelligence continues its meteoric rise, a quiet revolution is brewing beneath the headlines.
While the tech world obsesses over larger models and faster inference, a more fundamental shift is
occurring in how AI systems approach reasoning. The Chain-of-Thought (CoT) paradigm represents the
transition from black-box intuition to structured, transparent thinking processes- a development that
might signal the most significant advancement in AI since the transformer architecture.
From Intuitive Guesses to Strategic Reasoning
Traditional language models operate like savants: impressive yet often inscrutable. They provide
answers without explanation, making them unreliable partners for critical applications that demand
transparency and verification. This black-box approach creates a fundamental trust issue, particularly in
domains where reasoning matters as much as the final answer.
DeepSeek R1 exemplifies the next evolutionary step in AI reasoning. By implementing sophisticated
Chain-of-Thought mechanisms, R1 transforms from an answer generator into a reasoning partner that
reveals its computational journey step by step. This capability was dramatically demonstrated when R1
tackled complex International Mathematical Olympiad (IMO) problems-not merely providing solutions
but constructing rigorous proofs through explicitly articulated logical pathways.
Such transparency isn’t merely academic; it represents a fundamental shift in human-AI collaboration
across industries from healthcare to scientific research, where understanding the “why” behind
recommendations often matters more than the recommendations themselves.
The Three Pillars of Structured Reasoning
A) Instruction Engineering: Setting the Cognitive Stage
Effective CoT implementation begins with precise task framing. Unlike simplistic prompts that request
answers directly, sophisticated instruction engineering establishes both the boundaries of inquiry and
expectations for output format.
DeepSeek R1’s innovation lies in its Dynamic Prompt Tuner -an automated system that calibrates
instruction granularity based on problem complexity. For mathematical proofs, the system might
automatically trigger templates for proof by contradiction or mathematical induction. This dynamic
adaptation allows R1 to match its approach to the specific demands of the problem, significantly
outperforming static prompting methods.
B) Rationale Generation: Exposing the Thought Process
At CoT’s core is the explicit recording of intermediate reasoning steps. Rather than jumping directly to
conclusions, models like DeepSeek R1 document hypothesis generation, theorem application, and
logical validation-creating a transparent audit trail that mirrors expert human reasoning.
R1 implements this through a reinforcement learning mechanism based on the Generalized Reward-
Penalized Optimization (GRPO) algorithm. This approach allows the model to self-correct during
reasoning, creating a feedback loop that progressively refines its thinking pathway.
C) Strategic Exemplar Selection: Learning from Similar Problems
The third pillar involves leveraging semantic similarity to identify and apply relevant solution patterns
from analogous problems-even across different domains. This cross-disciplinary transfer enables models
to apply physics solution strategies to economic problems or medical diagnostic approaches to novel
diseases.
DeepSeek R1’s Hybrid Embedding technology represents a significant breakthrough, reducing traditional
CoT’s overdependence on domain-specific examples. This allows the model to reason effectively even in
domains with limited training examples, dramatically expanding its practical applicability.
Image Credit: Author
The Evolution of Thought Structures: From Chains to
Hypergraphs
A) Tree of Thoughts (ToT): Exploring Multiple Solution Paths
As reasoning challenges grow more complex, linear chains often prove insufficient. Tree of Thoughts
expands reasoning into a dendritic structure supporting both breadth-first and depth-first exploration of
solution spaces.
DeepSeek R1 applies ToT algorithms in competitive programming scenarios, generating multiple
algorithm proposals for the same problem, then systematically testing each through unit tests to
identify optimal solutions. This multi-path exploration mirrors how human experts mentally simulate
different approaches before committing to a specific solution strategy.
Agentic reasoning approaches
Friday, June 27, 2025 4:14 AM
Programming Page 44
Image Credit: Vrunda Gadesha
Zoom image will be displayed
Image Credit: Vrunda Gadesha
B) Graph of Thoughts (GoT): Dynamic Reasoning Networks
Moving beyond hierarchical trees, Graph of Thoughts introduces a more flexible network structure
incorporating self-loop connections that enable reinforced memory of key reasoning nodes.
In medical diagnosis scenarios, R1 demonstrates GoT’s power by merging insights from imaging analysis
with pathology reports, creating cross-validated conclusions that integrate multiple evidence streams.
This represents a significant advancement over linear reasoning chains that struggle to integrate
disparate information sources.
C) Program of Thoughts (PoT): Code as Intermediate Reasoning
Perhaps the most powerful evolution in the thought structure family is the Program of Thoughts, which
transforms mathematical reasoning into executable code segments that can be verified through running
results.
R1 showcases this capability in fluid dynamics problems by generating SymPy symbolic computation
code that automatically outputs analytical solutions to partial differential equations. This integration of
symbolic computation with natural language reasoning creates a powerful hybrid that exceeds the
capabilities of either approach alone.
Zoom image will be displayed
Image Credit: Author
Programming Page 45
Image Credit: Author
Image Credit: Matthew Mayo
DeepSeek R1’s Engineering Innovations: The Hybrid Reasoning
Architecture
A) Multi-Layered Reasoning Engine
DeepSeek’s implementation represents a sophisticated integration of multiple reasoning paradigms
through a layered architecture:
Base layer: Linear CoT for straightforward problems requiring minimal branching•
Middle layer: ToT multi-path exploration for problems with distinct solution approaches•
Top layer: GoT dynamic aggregation for complex scenarios requiring cross-domain synthesis•
This hierarchy enables dynamic resource allocation, with simpler problems processed entirely at lower
levels while complex challenges engage the full stack. This efficiency helps R1 maintain real-time
performance even when tackling computationally intensive reasoning tasks.
B) Dual-Objective Reward Function Design
R1’s training regime balances dual objectives through carefully designed reward functions:
Accuracy rewards focus on the correctness of final answers•
Explainability rewards evaluate the coherence and logical validity of reasoning chains•
C) Cross-Modal Extensions
Perhaps most impressively, DeepSeek R1 extends Chain-of-Thought beyond text to include visual
reasoning through a structured process:
Object identification in images1.
Spatial relationship mapping2.
Semantic inference based on visual elements3.
This multimodal CoT capability enables R1 to solve reasoning problems that combine textual and visual
information critical capability for applications ranging from autonomous vehicles to medical imaging
analysis.
From Tools to Thinking Partners: The New Paradigm
The CoT revolution transforms AI from answer generators to thinking process collaborators,
fundamentally reshaping human-machine interaction across education, research, and professional
domains. When students can observe AI’s step-by-step problem-solving approach rather than merely
receiving answers, learning becomes collaborative rather than prescriptive.
From a philosophical perspective, as machines begin to demonstrate traceable thinking pathways, our
understanding of intelligence itself enters a new era. The transparent nature of Chain-of-Thought
approaches may ultimately lead us to recognize that true intelligence isn’t about having answers, but
rather about having explicable methods for deriving them.
As DeepSeek R1 and similar systems continue to advance, we stand at the threshold of an era where AI
becomes not merely a tool but a genuine thinking partner-capable of not just what to think, but
revealing how it thinks. This transparency may ultimately prove more valuable than any specific
capability, as it transforms AI from a black box into a glass box, allowing humans to learn from, verify,
and collaborate with artificial reasoning systems in unprecedented ways.
From <https://medium.com/ai-simplified-in-plain-english/deepseek-r1s-cot-revolution-rethinking-ai-reasoning-from-chains-to-
hypergraphs-2baecbb5fe26>
Programming Page 46
https://victormorgante.medium.com/formalism-for-game-theoretic-approach-to-formal-logic-
cbf1b043b68d
Agent learning
Thursday, June 26, 2025 5:18 PM
Programming Page 47
https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-
using-large-language-models/
Evolutionary Agents and Algorithms
Sunday, June 8, 2025 10:15 AM
Programming Page 48
https://medium.com/data-science-collective/why-most-ai-agents-fail-in-production-and-how-to-build-
ones-that-dont-f6f604bcd075
Agents in production roadmap
Thursday, June 26, 2025 4:57 PM
Programming Page 49
https://www.anthropic.com/engineering/built-multi-agent-research-system
Anthropic multiagent research
Thursday, June 26, 2025 4:58 PM
Programming Page 50
Programming Page 51
https://seanfalconer.medium.com/agentic-mesh-the-future-of-enterprise-agent-ecosystems-
f26e7d53951e
SUMMARY: "Agentic Mesh — The Future of Enterprise Agent Ecosystems" by Sean Falconer et
al.
CORE ARGUMENT
The article proposes a shift from isolated AI agents to enterprise-grade, interoperable agent
ecosystems—termed the agentic mesh. This architecture aims to transition agents from ad hoc
prototypes into governable, secure, observable, and scalable components of enterprise
infrastructure.
WHY ENTERPRISES NEED AI AGENTS
Agents promise to:
Alleviate information overload by surfacing relevant insights automatically.•
Increase operational efficiency by automating complex workflows.•
Improve customer experience via personalized, context-aware interactions.•
Free human labor for strategic innovation by handling repetitive tasks.•
THE PROBLEM
Agentic mesh
Thursday, June 26, 2025 5:20 PM
Programming Page 52
THE PROBLEM
Most agents today are:
Prototype-grade, lacking robustness or deployability.•
Monolithic and unscalable, often single-process Python scripts.•
Opaque and insecure, with no observability, traceability, or access control.•
Siloed, unable to interoperate with other systems or agents.•
Over-reliant on LLMs, making them brittle and non-deterministic.•
THE SOLUTION: THE AGENTIC MESH
The agentic mesh is an architectural framework for creating a networked ecosystem of agents
that can collaborate, transact, and operate securely in production environments.
Key Components:
Marketplace: Discovery and reuse of prebuilt agents.•
Registry: Agent authentication, identity, and discoverability.•
Observability & Governance: Logging, metrics, access policies, audit compliance.•
Communication & Orchestration: Coordinated workflows via LLMs and deterministic
engines.
•
Additional systems include:
Interaction Manager: Manages human and agent communication via APIs or chat.•
Creator Workbench: Development toolkit for production-ready agents.•
CHARACTERISTICS OF ENTERPRISE-GRADE AGENTS
Discoverable: Documented, registered with unique metadata.•
Secure: Uses mTLS, OAuth2, and zero-trust access models.•
Observable: Emits logs/metrics compatible with enterprise monitoring tools.•
Reliable: Minimizes stochastic behaviors; uses deterministic execution where possible.•
Scalable: Architected for runtime elasticity and ease of development.•
Trustworthy: Certified before deployment.•
Traceable: Action-justification logging for diagnostics and compliance.•
Collaborative: Built for multi-agent and multi-tool ecosystems.•
TECHNICAL UNDERPINNINGS
Architectural Model:
Agents are smart microservices:
Deployed via Docker/Kubernetes.•
Observed with OpenTelemetry/Prometheus.•
Governed with enterprise-grade tooling.•
Operational Design:
Tool-Driven: Agents select and use tools/APIs autonomously.•
Conversational: Manage extended, stateful dialogues.•
Asynchronous: Modeled like human communication (e.g., emails, messages).•
Event-Driven: Agents use systems like Kafka/Flink to publish/subscribe to events.•
Governance:
Zero-trust enforcement: Strict control over agent data/tool access.•
Runtime policies encoded in mesh metadata and access protocols.•
CONCLUSION
The future of enterprise AI isn't just more agents—it's better agents: interoperable, secure, and
managed at scale. The agentic mesh serves as the infrastructure backbone enabling this
transition, moving agents from the lab to mission-critical roles in enterprise software architecture.
Programming Page 53
https://medium.com/ai-simplified-in-plain-english/deepseek-r1s-cot-revolution-rethinking-ai-reasoning-
from-chains-to-hypergraphs-2baecbb5fe26
Hypergraph chain of thought
Friday, July 25, 2025 12:36 AM
Programming Page 54
https://github.com/open-thought/system-2-research
System 2 research
Friday, July 25, 2025 12:42 AM
Programming Page 55