DARWIN GÖDEL MACHINE: OPEN-ENDED EVOLUTION
OF SELF-IMPROVING AGENTS
Jenny Zhang*,1,2 Shengran Hu*,1,2,3 Cong Lu1,2,3 Robert Lange†,3 Jeff Clune†,1,2,4
1University of British Columbia 2Vector Institute 3Sakana AI 4Canada CIFAR AI Chair
{jennyzzt,srhu,conglu}@cs.ubc.ca, robert@sakana.ai, jeff.clune@ubc.ca
ABSTRACT
Most of today’s AI systems are constrained by human-designed, fixed architectures
and cannot autonomously and continuously improve themselves. The scientific
method, on the other hand, is a cumulative and open-ended system, where each
innovation builds upon previous artifacts, enabling future discoveries. There is
growing hope that the current manual process of advancing AI could itself be
automated. If done safely, such automation would accelerate AI development and
allow us to reap its benefits much sooner. This prospect raises the question of
how AI systems can endlessly improve themselves while getting better at solving
relevant problems. Meta-learning can automate the discovery of novel algorithms,
but is limited by first-order improvements and the human design of a suitable search
space. The Gödel machine (Schmidhuber, 2007) proposed a theoretical alternative:
a self-improving AI that repeatedly modifies itself in a provably beneficial manner.
Unfortunately, proving that most changes are net beneficial is impossible in practice.
We introduce the Darwin Gödel Machine (DGM), a novel self-improving system
that iteratively modifies its own code (thereby also improving its ability to modify
its own codebase) and empirically validates each change using coding benchmarks.
Inspired by Darwinian evolution and open-endedness research, the DGM grows
an archive of generated coding agents. It samples agents from this archive, which
self-modify to create new, interesting versions of themselves. This open-ended
exploration forms a growing tree of diverse, high-quality agents and allows the
parallel exploration of many different paths through the search space. Empirically,
the DGM automatically improves its coding capabilities (e.g., better code editing
tools, long-context window management, peer-review mechanisms), increasing
performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to
30.7%. Furthermore, the DGM significantly outperforms baselines without self-
improvement or open-ended exploration. All experiments were done with safety
precautions (e.g., sandboxing, human oversight). Overall, the DGM represents a
significant step toward self-improving AI, capable of gathering its own stepping
stones along a path that unfolds into endless innovation. All code is open-sourced
at https://github.com/jennyzzt/dgm.
1 INTRODUCTION
Scientific progress is cumulative and open-ended, with each breakthrough standing on the shoulders
of countless prior insights. In the same way, our most advanced AI systems are built upon a long
lineage of innovations. For instance, transformers (Vaswani et al., 2017), the backbone of current large
language models (LLMs) (Brown et al., 2020), did not emerge in isolation but were built upon years
of past innovations, such as recurrent neural networks (Linnainmaa, 1970; Amari, 1972; Hopfield,
1982; Rumelhart et al., 1985) and attention mechanisms (Schmidhuber & Huber, 1990; Bahdanau
et al., 2015; Kim et al., 2017; Parikh et al., 2016). However, most of today’s AI systems remain bound
by fixed, human-designed architectures that learn within predefined boundaries, without the capacity
to autonomously rewrite their own source code to self-improve. As a result, each advancement in AI
development still leans heavily on human interventions, tethering the pace of progress. This paper
*co-authors † co-senior authors
1
arXiv:2505.22954v2 [cs.AI] 26 Sep 2025
investigates the intriguing possibility of safely automating the search for ever-better AI. One can
imagine an AI system that, like scientific discovery itself, becomes an engine of its own advancement:
building upon its past, recursively improving, and propelling itself toward more advanced capabilities.Archive
New
coding
agent
Coding
agent
Self-modify
child
Coding agent’s
own repo
+
Self-improve
instruction
Code Diff:
Feature to
improve itself
Coding
agentselect
Task repo
(e.g., GitHub repo)
+
Task instruction
(e.g., GitHub issue)
Code Diff:
Solve task
New
coding
agentEvaluate on
benchmark
add
parent
Figure 1: Darwin Gödel Machine. The DGM iteratively builds a growing archive of agents by
interleaving self-modification with downstream task evaluation. Agents in the archive are selected for
self-modification through open-ended exploration.
Schmidhuber (2007) presented a class of mathematically rigorous, self-referential, self-improving
problem solvers. It relies on formal proofs to justify code rewrites, ensuring that any self-modification
is provably beneficial. However, in practice and without restrictive assumptions about the system,
it is impossible to formally prove whether a modification to an AI system will be beneficial. For
example, while it may seem that an LLM-based coding agent would benefit from access to more
tools (e.g., code search, test runners), the actual impact depends heavily on the model’s training and
task context (e.g., a testing tool that is optimized for one setup may confuse the agent when working
with others). Instead of requiring formal proofs, we empirically validate self-modifications against a
benchmark, allowing the system to improve and explore based on observed results. This approach
mirrors biological evolution, where mutations and adaptations are not verified in advance but are
produced, trialed, and then selected via natural selection. We also take inspiration from Darwinian
evolution (Darwin, 2023) and investigate the effectiveness of maintaining a library of previously
discovered agents to serve as stepping stones for future generations.
We propose the Darwin Gödel Machine (DGM), a self-referential, self-improving system that
writes and modifies its own code to become a better coding agent. Each self-modification requires
the DGM to edit its own codebase. We use Python, which is Turing-complete, giving the DGM
the potential to build any computable machine. Our framework envisions agents that can rewrite
their own training scripts (including training a new foundation model (FM)). However, we do not
show that in this paper, as training FMs is computationally intensive and would introduce substantial
additional complexity, which we leave as future work. Instead, this paper focuses on improving the
design of coding agents with frozen pretrained FMs (e.g., tool use, workflows). The DGM alternates
between self-modification and evaluation phases. During the self-modification phase, selected coding
agents from the archive generate modified versions of themselves. During the evaluation phase,
each modified agent is tested on a coding benchmark, estimating the agent’s coding capabilities, and
then added to the archive. By improving its own capabilities through this loop, the DGM becomes
better at both solving coding tasks and making future self-improvements. A key assumption is
that an increase in performance on coding benchmarks indicates better coding capabilities, and
hence better ability to self-modify and self-improve. Furthermore, the DGM maintains an archive of
generated coding agents, initialized with only one agent, and continuously accumulates all generated
variants over time. To support continual self-improvement, the DGM draws inspiration from open-
endedness research (Wang et al., 2019; Fernando et al., 2024; Faldor et al., 2025), accumulating
diverse stepping stones (i.e., interesting yet suboptimal solutions or features that may enable future
breakthroughs). This open-ended exploration encourages the discovery of novel and potentially
useful self-modifications beyond immediate performance gains.
We present results on two coding benchmarks: SWE-bench (Jimenez et al., 2024) and Polyglot (Paul
Gauthier, 2024). The DGM automatically improves itself from 20.0% to 50.0% on SWE-bench,
and from 14.2% to 30.7% on Polyglot. We show that self-improvement enables continued progress,
as the DGM outperforms the baseline where the same base agent is repeatedly used to modify and
generate new agents without self-improvement. We also show that open-ended exploration and
2
keeping an archive of all previously generated agents lead to the discovery of better coding agents.
The DGM outperforms the baseline of not having open-ended exploration (i.e., a baseline without the
accumulation of an archive of interestingly different stepping stones), where the coding agent always
builds off the most recent version of itself. Overall, the DGM represents a step toward AI systems
that can build upon their own prior innovations and improve recursively. We consider and discuss
safety aspects extensively, including sandboxing and traceability of self-modifications, to ensure
responsible experimentation (Section 5). By advancing the possibility of safe, self-referential, self-
improving models, the DGM moves us closer to AI that not only learns but evolves in an open-ended,
self-accelerating trajectory, much like science itself.
2 RELATED WORK
Open-Endedness. A grand challenge for driving unbounded innovation is designing open-ended AI
systems that continuously generate novel and learnable artifacts (Stanley et al., 2017). Hughes et al.
(2024) characterized open-endedness as a system’s capacity to generate sequences of artifacts that are
both novel and learnable from an observer’s perspective. A central difficulty lies in structuring and
exploring vast search spaces to consistently produce artifacts that are interesting to humans (Clune,
2019; Jiang et al., 2023). Early progress drew on quality-diversity algorithms, goal-directed ex-
ploration, intrinsic motivation, and learning-progress frameworks (Pugh et al., 2016; Ecoffet et al.,
2019; Lehman & Stanley, 2011; Oudeyer et al., 2007), while recent advances leverage large-scale
foundation models (FMs) as proxies for human interestingness and versatile engines for generating
and evaluating novel behaviors across diverse domains (Brown et al., 2020; Hu et al., 2025; Zhang
et al., 2024b). However, these approaches have yet to close the self-referential self-improvement
loop, meaning improvements on downstream tasks do not translate into enhanced capabilities for
self-modification or the acceleration of further innovations. We aim to mimic the acceleration of sci-
ence and technology, where new tools and discoveries catalyze the creation of even more discoveries.
How can we emulate nature’s arc of evolution, which bends not only toward complexity but also an
ever greater capacity to evolve (Dawkins, 2019; Gerhart & Kirschner, 2007; Hendrikse et al., 2007)?
Meta-Learning FM Agents. Many FM-based agents are handcrafted. Some building blocks include
prompt engineering (Chen et al., 2023; Schulhoff et al., 2024), chain-of-thought (Wei et al., 2022;
Yao et al., 2023; Hu & Clune, 2024; Guo et al., 2025; Lightman et al., 2023; Muennighoff et al., 2025;
Zelikman et al., 2024a), self-reflection (Shinn et al., 2023; Yao et al., 2023; Madaan et al., 2023),
multi-agent debate (Zhuge et al., 2023; Liang et al., 2023; Khan et al., 2024), memory (Liu et al., 2023;
Zhong et al., 2024; Modarressi et al., 2023), temperature sampling (Zhu et al., 2024), and retrieval
augmented generation (Lewis et al., 2020). The manual composition of these components limits
the system’s abilities to the ingenuity of its human designer. More recently, several meta-learning
approaches have emerged that leverage FM to automatically optimize prompts (Fernando et al., 2024;
, FAIR; Khattab et al., 2023; Cheng et al., 2024; Yuksekgonul et al., 2024; Yuan et al., 2024) and
design agentic modules (Zhang et al., 2024c; Zhou et al., 2024; Yin et al., 2024; Zhuge et al., 2024;
Rosser & Foerster, 2025; Zhang et al., 2025a; Ye et al., 2025; Gao et al., 2025; Nie et al., 2025;
Su et al., 2025; Zhang et al., 2025b; Niu et al., 2025). The Automated Design of Agentic Systems
(ADAS, Hu et al., 2025) iteratively generates downstream agents with a fixed meta-agent, evaluates
them against a target benchmark, and incorporates feedback to refine subsequent generations. In
contrast, the DGM is a single system that both solves downstream tasks (i.e., coding problems)
and refines its own implementation (i.e., its codebase), removing the need for a fixed, handcrafted
meta-agent and enabling self-referential improvements.
Self-Improving AI. Early on, various researchers outlined theoretical and conceptual approaches to
self-improvement (Good, 1966; Schmidhuber, 1987; 2007). Some practical approaches to automated
self-improvement include systems defined by neural network weight parameterizations (Schmidhuber,
1993; Hall, 2007; Hobbhahn, 2025; Kirsch & Schmidhuber, 2022; Irie et al., 2022; 2025; Lu et al.,
2023; Havrilla et al., 2024b). Metz et al. (2021) developed a gradient-based optimizer that is self-
referentially meta-trained using a variant of population-based training (Jaderberg et al., 2017). Lange
et al. (2023) extended this approach to gradient-free learning. Silver et al. (2017) used self-play
to continuously evolve agents, achieving superhuman performance in challenging domains such
as chess and Go. More closely related to the DGM are recent approaches that leverage FM-based
agents for self-improvement (Yin et al., 2024; Robeyns et al., 2025; Hu et al., 2024; Zelikman et al.,
2024b; Huang et al., 2022; Singh et al., 2023). Zelikman et al. (2024b) use a meta-agent to generate
3
downstream agents, updating the meta-agent based on the meta-utility derived from the generated
solutions. Yin et al. (2024) use a single system to both solve downstream tasks and recursively modify
itself. However, the downstream tasks or the meta-utility do not always align with the capabilities
required for self-improvement. In the DGM, improvement in downstream tasks directly reflects
an increase in self-improvement ability, enabling the potential for self-accelerating progress. Most
similar is concurrent work by Robeyns et al. (2025), which also has a single agent recursively solving
coding problems and modifying its own codebase. The main difference from Robeyns et al. (2025) is
that the DGM has an open-ended exploration loop, encouraging self-modifications beyond immediate
performance gains and thus avoiding stagnation in suboptimal states. Appendix B also discusses
additional related work on program synthesis and Darwinian evolution.
3 DARWIN GÖDEL MACHINE
A Gödel Machine is a theoretical idea of an AI that searches for ways that provably improve
itself (Schmidhuber, 2007). In this paper, we propose Darwin Gödel Machine (DGM), an attempt to
realize the long-held dream of creating a Gödel Machine. The DGM relaxes the Gödel Machine’s
impractical requirement of theoretically proving that a change will improve the system, instead
requiring empirical evidence from experiments to demonstrate that a proposed new version enhances
performance. Additionally, since the DGM relies on empirical evidence of improvement, it may
get stuck in a local optimum within the vast search space of possible systems (i.e., all computable
algorithms). To address this, the DGM maintains an archive of discovered solutions during the
search, facilitating open-ended exploration rather than relying on evolving a single solution. Since the
principles echo Darwinian evolution (Darwin, 2023) (Appendix B), where new innovations emerge
by selecting an entity from an archive of previously discovered solutions, modifying it, and keeping
it if it is interestingly new (Zhang et al., 2024b; Faldor et al., 2025; Stanley & Lehman, 2015), we
call our algorithm a Darwin Gödel Machine (Figure 1).
Self-referential Self-improvement of Coding Agents. The DGM is initialized with only one coding
agent, and its progression is evaluated on coding benchmarks. A coding agent is defined as a single
system, implemented with a code repository and powered by frozen pretrained foundation models
(FMs), capable of reading, writing, and executing code. Code, when expressed in a general-purpose
Turing-complete language (e.g., Python), is a powerful medium for building and improving intelligent
systems because it can represent any computable process. Recent works (Hu et al., 2025; Zhang et al.,
2024c) demonstrate that such agents can be improved through meta-learning of their designs (e.g.,
prompts, workflows, and tools), which are implemented in their code repository. Therefore, we define
self-improvement as a coding task that involves modifying the design of an agent’s own components
(i.e., its own code, which does not include the open-ended exploration process described in the next
paragraph). The key motivation is that the empirical evidence must reflect the system’s ability to both
self-improve and solve downstream tasks. By configuring the DGM as a coding agent and testing
its coding capabilities, the observed improvements demonstrate not only enhanced performance
in downstream tasks but also the capacity for further self-improvement, as self-improvement is
fundamentally a coding task that modifies the coding agent’s own code repository.
Population-based Open-ended Exploration. Starting from a single initial coding agent, the DGM
builds an archive of all discovered agents. In each iteration, the DGM selects parent agents to
self-modify and branch off to produce new agents. Parent selection is roughly proportional to
each agent’s performance score and the number of its children with codebase-editing functionality
(Appendix C.2). Each selected parent analyzes its own benchmark evaluation logs, proposes the next
feature to implement, and receives this proposal as a problem statement to execute (Appendix C.3).
The parent then implements the suggested feature into its own codebase, generating a new coding
agent. Each newly generated agent is quantitatively evaluated on a chosen coding benchmark to
estimate its coding abilities. Only agents that compile successfully and retain the ability to edit a
given codebase are added to the DGM archive, as only they can continue self-modification. All
others are discarded. The cycle of parent selection, self-modification, and evaluation continues,
progressively growing the archive of solutions. Importantly, we note that archived solutions can serve
as stepping stones that result in improvements much later than their original discovery, making our
approach substantially different from hill-climbing agentic design approaches (Robeyns et al., 2025).
Currently, the open-ended exploration process (i.e., archive maintenance, parent selection) is fixed
4
and not modifiable by the DGM, which we leave as an avenue for future work. Appendix C.4 shows
the pseudocode for the DGM algorithm.
4 EXPERIMENTS
Section 4.1 describes the experimental setup, including the initial coding agent that initializes the
DGM. We conduct experiments on two coding benchmarks: SWE-bench (Jimenez et al., 2024) and
Polyglot (Paul Gauthier, 2024) (Section 4.2). For each benchmark, we compare the DGM against
two baselines: DGM without self-improving agents and DGM without open-ended exploration
(Section 4.3). Across all experiments, we find that the DGM outperforms both baselines, showing that
the self-improvement mechanism and open-ended exploration are essential for sustained performance
gains (Section 4.4). Furthermore, we show that the features discovered by the DGM transfer across
models, benchmarks, and tasks (Section 4.4).
4.1 EXPERIMENT SETUP
The DGM is initialized with a single coding agent. This base agent is built around a frozen FM and
augmented with tool use capabilities (Schick et al., 2023; Anthropic, 2024b). The FM autonomously
determines its action at each step, using whichever tool it deems appropriate. The base agent has
access to two tools: a Bash tool for executing bash commands, and an edit tool for viewing and editing
directories and entire files (Appendix C.1). The agent receives a single input prompt that specifies
the location of the target repository, the problem statement, and the repository’s testing framework
(Appendix C.1). We run the DGM for 80 iterations (generating one new agent per iteration), with
two iterations running in parallel for SWE-bench and four for Polyglot (Appendix C.2). During
self-modifications, coding agents are powered by Claude 3.5 Sonnet (New) (Anthropic, 2024a) in both
SWE-bench and Polyglot experiments. During benchmark evaluation, coding agents are powered by
Claude 3.5 Sonnet (New) for SWE-bench and o3-mini (OpenAI, 2025) for Polyglot (Appendix D.1).
4.2 BENCHMARKS
Both SWE-bench and Polyglot are benchmarks that evaluate how well AI agents solve coding
tasks automatically. Given a code repository and a task instruction, the agent is expected to make
changes to the repository in order to fulfill the task. Both SWE-bench and Polyglot are widely used
benchmarks (Zhang et al., 2024d;a; Xia et al., 2024; Cao et al., 2024; Google DeepMind, 2025;
Gauthier, 2024) that require the AI agent to navigate a code repository, understand the interplay
between functions in different files, and spot small errors in convoluted code. SWE-bench only has
Python tasks, while Polyglot has tasks in multiple programming languages. Another difference is
that each SWE-bench task may require edits to multiple files, whereas each Polyglot task primarily
involves implementing a solution from scratch in a single file (although the agent still needs to
examine other files to understand what changes are necessary), resulting in fewer file edits overall.
We evaluate the DGM on two popular benchmarks that assess different aspects of coding tasks to
validate the algorithm’s effectiveness across various use cases.
SWE-bench. To avoid wasting compute on unsolvable tasks, we use SWE-bench Verified (OpenAI,
2024), a human-filtered subset of SWE-bench (Jimenez et al., 2024) where all tasks are solvable.
Throughout this paper, the term SWE-bench refers by default to to the SWE-bench Verified subset.
Polyglot. Polyglot includes tasks in multiple programming languages (C++, Rust, Python, etc.) (Paul
Gauthier, 2024). Compared to SWE-bench, one of the most widely used coding benchmarks and
likely included in the training sets of FMs, Polyglot is more niche and less likely to be included
in FMs’ post-training data. Additionally, Polyglot is primarily used by its developer to evaluate
Aider (Gauthier, 2024). This provides an opportunity to compare automatically designed agents with
a representative coding agent in the community, which has been continuously developed and tested
against the Polyglot benchmark over a year by human developers. Finally, Polyglot enables testing of
the DGM’s ability to generalize across multiple programming languages beyond just Python. We
follow the Polyglot leaderboard setup (Paul Gauthier, 2024), with one key difference: the leaderboard
reports pass@2, where the agent can view feedback from ground-truth tests once, whereas we report
pass@1, where the agent never sees ground-truth test results.
5
Since it is expensive to evaluate on the full benchmarks (Appendix E.1), we use a staged evaluation
strategy to estimate the coding agent’s performance. In general, we evaluate the coding agent on
more tasks when we are more confident that it is a relatively strong performer. We first evaluate
each coding agent on a set of 10 tasks to verify basic codebase-editing functionality. Only agents
that retain the ability to edit code can solve downstream coding tasks or perform further self-
modifications. Agents that pass this initial stage are then evaluated on an expanded set of 50 tasks.
For SWE-bench, tasks are selected based on SWE-bench-verified-mini (Hobbhahn, 2025), which is
designed to be a representative sample of the full benchmark (Appendix E.2). For Polyglot, tasks
are chosen as a random subset of the full benchmark (Appendix E.3). Because the LLMs we use
are inherently stochastic, performance can be noisy. We use smaller subset evaluations to estimate
general effectiveness and identify top candidates. For SWE-bench, if a coding agent achieves over
40% success (a heuristic chosen based on the noise observed in preliminary runs) on the 60-task
subset and ranks among the top two performers in the archive, it is then evaluated on 200 tasks
(140 plus the previous 60) to more accurately assess its true coding capability (Appendix E.2). For
Polyglot, if a coding agent achieves over 40% success on the 10-task subset, it is evaluated on an
expanded 50-task subset (Appendix E.3).
4.3 BASELINES
To isolate and evaluate the contribution of the DGM’s core components, we compare the DGM against
two baselines: DGM without self-improving agents (DGM w/o self-improve) and DGM without
open-ended exploration (DGM w/o open-ended exploration). DGM w/o self-improve replicates the
approach of ADAS (Hu et al., 2025) in this setting, wherein the meta agent responsible for modifying
the coding agents remains fixed as the base agent throughout the experiment (Algorithm 2). This
baseline allows us to assess the role of iterative self-improvement in accelerating the evolution of
better coding agents. DGM w/o open-ended exploration eliminates the use of an archive and always
self-modifies the latest stored version of itself (Algorithm 3). If a coding agent self-modifies to the
point where it loses the basic functionality required to edit a codebase, it can no longer modify itself
or solve any coding task. Therefore, DGM w/o open-ended exploration retains the latest version
of itself that still maintains the basic functionality for codebase editing. This baseline allows us to
evaluate the impact of having an archive and the well-documented beneficial principles of open-ended
exploration (Clune, 2019; Stanley & Lehman, 2015; Zhang et al., 2024b; Fernando et al., 2024; Lee
et al., 2020; Samvelyan et al., 2024; Colas et al., 2022b) in guiding the agent’s evolution.
In addition to the learned baselines, we compare the DGM against handcrafted, open-source solutions.
For SWE-bench, we take the state-of-the-art (SoTA) open-source solution that has been checked
(i.e., the SWE-bench team was able to reproduce the results) (Appendix E.4). For Polyglot, we take
the representative agent (Aider) (Gauthier, 2024), which is open-sourced and designed to support
multiple programming languages and large codebase editing (Appendix E.5). For a fair comparison,
we measure the percentage of solved tasks on the same benchmark subsets used to evaluate the DGM
(Appendix E.2, Appendix E.3). These values are shown as dotted horizontal lines in Figure 2.
4.4 RESULTS
After 80 iterations of the DGM, the coding agent’s performance increases from 20.0% to 50.0% on
SWE-bench, and from 14.0% to 38.0% on Polyglot (Figure 2). Since the DGM is evaluated on only
50 tasks in the Polyglot experiment setup (Section 4.2), we additionally evaluate both the base agent
and the best DGM-discovered agent on the full Polyglot benchmark to more accurately estimate the
improvement. On the full Polyglot benchmark, the DGM improves the coding agent from 14.2% to
30.7%. This shows that the DGM can automatically self-improve to create a better coding agent.
Moreover, the performance of the best DGM-discovered agent is comparable to that of the checked,
open-source, human-designed SoTA on SWE-bench (Figure 2). On Polyglot, although the DGM
starts with a base agent whose performance is lower than that of Aider, it discovers an agent that far
surpasses Aider (Figure 2). The DGM-discovered agents are comparable to or outperform handcrafted
agents on both benchmarks. While the SoTA SWE-bench agent and Aider were painstakingly shaped
by human efforts, the DGM hints at a future in which such ingenuity is automated, evolving through
self-referential cycles of continuous self-improvements.
6
0 10 20 30 40 50 60 70 80
Iterations
0.20
0.25
0.30
0.35
0.40
0.45
0.50
SWE-bench Score of Best Agent Checked Open-sourced SoTA
DGM w/o Open-ended exploration
DGM w/o Self-improve
DGM0 10 20 30 40 50 60 70 80
Iterations
0.15
0.20
0.25
0.30
0.35
Polyglot Score of Best Agent
Representative Agent Baseline (Aider)Figure 2: Self-improvement and open-ended exploration enable the DGM to continue making
progress and improve its performance. The DGM automatically discovers increasingly better
coding agents and performs better on both (Left) SWE-bench and (Right) Polyglot. It outperforms
baselines that lack either self-improvement or open-ended exploration, showing that both components
are essential for continual self-improvement. These scores are obtained from evaluating on the
benchmark subsets detailed in Section 4.2.SWE-bench
Score
DGM Archive Tree
10 tasks (no basic functionality)
60 tasks
200 tasks (score ≥ 0.4 and
top two of archive then)
Final best agentNon-empty patch
validation and retry
More granular file
viewing via lines
More granular file
editing via string
replacement
Auto-summarize
on context limit
Multiple patch
generations and
ranking
History-aware
patch generations
and ranking
DGM Progress
Figure 3: The DGM automatically self-improves to become a better coding agent. (Left) Archive
of coding agents generated during the DGM run on SWE-bench. Each node represents a coding
agent, with node 0 corresponding to the base agent. Node color indicates performance on SWE-bench
(percentage of solved tasks), while border color reflects the number of tasks for which the agent
was evaluated. Edges show which agents self-modified to produce the offsprings. Many paths to
innovation traverse lower-performing nodes, and key innovations (like node 24) lead to an explosion
of innovations built on top of them. Both properties underscore the benefits of open-ended search.
(Right) Progress plot of the DGM on SWE-bench. The light blue line shows the average score of all
agents possessing basic codebase-editing functionality. The blue line tracks the best score achieved by
any agent in the archive at each iteration. The dark line shows the lineage of the final best-discovered
agent and its precursor nodes, which includes two performance dips. This illustrates the benefits of
open-ended search, which explores a diverse set of interesting stepping stones instead of focusing
only on branching off the best solution found so far.
The DGM automatically improves both the tools and the workflow of how FMs are utilized (Figure 3).
For example, the DGM enhanced the edit tool to allow more granular file viewing (by lines) and
more precise file editing (by string replacement), instead of always viewing or replacing the entire
file. Workflow improvements include making multiple attempts to solve a task and using another FM
to evaluate and select the best solution. Other workflow improvements include considering previous
attempts when generating subsequent ones. Appendix F.1 and Appendix F.2 show all modifications
leading up to the final best-discovered agents on SWE-bench and Polyglot respectively.
7
Because open-ended exploration allows branching from any agent in the archive with non-zero
probability, the DGM can get out of deceptive dips or peaks in performance. For example, at
iterations 4 and 56 of the experiment on SWE-bench, although the agent’s score temporarily fell
below that of its parent, the DGM was still able to explore innovations along that path and create a
new agent that outperformed all of its predecessors (Figure 3). Furthermore, open-ended exploration
allows different implementations of the same target functionality to be attempted. For example, while
the goal is to provide finer-grained editing tools, the specific implementation of this feature can vary
greatly and hence lead to very different performance (Appendix G). The DGM can explore multiple
implementations to find the most suitable one and avoid getting trapped in a suboptimal one.
The DGM outperforms the baselines of DGM w/o self-improve and DGM w/o open-ended exploration
on both benchmarks (Figure 2). Without updating the meta agent that modifies coding agents, DGM
w/o self-improve improves the agents in early iterations, but its gains taper off quickly (Appendix A.1).
In DGM w/o open-ended exploration, only the most recent agent is retained, so a poorly performing
self-modification makes subsequent improvements harder to achieve (Appendix A.1).o3-mini Claude 3.5 Sonnet Claude 3.7 Sonnet
0
10
20
30
40
50
60
Success Rate on SWE-bench (%)
23.0% 20.0% 19.0%
33.0%
50.0%
59.0%
Model Transfer on SWE-bench
SWE-bench Polyglot
0
10
20
30
40
50
60
Success Rate (%)
20.0%
14.2%
24.5%
28.9%
50.0%
30.7%
Zero-shot Transfer Between Benchmarks
python task non-python task
0
5
10
15
20
25
30
35
40
Success Rate on Polyglot (%)
8.8%
15.2%
11.8%
17.3%17.6%
33.0%
20.6%
30.8%
Task Transfer on Polyglot
Base agent
Best agent searched with Claude 3.5 Sonnet
Best agent transfer to other FMs
Base agent
Best agent searched on SWE-bench
Best agent searched on Polyglot
Base agent
Aider
DGM search on all language
DGM search only on python
Figure 4: Transfer between Models, Benchmarks, and Tasks. The superior performance of
DGM-discovered agents can be transferred across (Left) different models, (Middle) benchmarks, and
(Right) different programming language tasks in Polyglot, such as from Python tasks to C++ tasks.
To evaluate the generality of the improvements from the DGM, we tested the base agent (Section 4.1)
and the best agent discovered during the DGM run (Figure 2) with different FMs than those used
during optimization. For SWE-bench, where the DGM was run using Claude 3.5 Sonnet (New),
we replaced the FM with Claude 3.7 Sonnet (Anthropic, 2025) or o3-mini, and evaluated on 200
tasks (Figure 4, Left). With o3-mini, the base agent achieved 23.0% and the DGM-discovered agent
33.0%. With Claude 3.7 Sonnet, the base agent achieved 19.0% and the DGM-discovered agent
59.5%. These results suggest that the DGM yields improvements that generalize across FMs, rather
than being tightly coupled to the specific FM used during its run (Figure 4). Model transfer results on
Polyglot are presented in Appendix A.2.
Furthermore, we investigate the transferability of the DGM-discovered agent across different bench-
marks and programming languages. First, we evaluate the best DGM-discovered agent from one
benchmark (e.g., SWE-bench) on a completely held-out benchmark (e.g., Polyglot), and vice versa
(Figure 4, Middle). The best agent evolved on SWE-bench achieves 28.9% on Polyglot, compared
to the initial agent’s baseline of 14.2%. Conversely, the best agent evolved on Polyglot achieves
24.5% on SWE-bench, outperforming the original baseline of 20.0%. Since each agent was optimized
without ever accessing the alternate benchmark, these evaluations represent truly held-out tests. The
consistent performance gains across benchmarks support our claim that DGM’s improvements reflect
general skill acquisition rather than overfitting or exploitation of benchmark-specific artifacts. Second,
we experiment with a version of the DGM trained exclusively on Python tasks from Polyglot and then
transfer the discovered agent to tasks in other languages. Focusing primarily on Python tasks slightly
improves performance on Python tasks but reduces performance on non-Python tasks compared to
the DGM trained on all languages (Figure 4, Right). However, after being transferred from Python to
other unseen languages during the search, the agent still achieves performance comparable to that
of the DGM trained on all languages and substantially outperforms both the base agent and Aider.
These results demonstrate the robustness of the discovered improvements, showing that they do not
overfit to a specific programming language. We also present additional results in Appendix A.
8
5 SAFETY DISCUSSION
Systems capable of self-improvement, such as the DGM, represent a step toward more autonomous
AI development, aligning with long-standing goals in the field of making capable AI that can benefit
humanity (Schmidhuber, 1987; Clune, 2019; Markoff, 2016; Lehman, 2023). However, this capability
introduces unique safety considerations stemming from the system’s ability to autonomously modify
its own code. Modifications optimized solely for benchmark performance might inadvertently
introduce vulnerabilities or behaviors misaligned with human intentions, even if they improve the
target metric (Bostrom, 2020). In particular, if evaluation benchmarks do not fully capture all desired
agent properties (e.g., safety and robustness), the self-improvement loop could amplify misalignment
over successive generations. Iterative self-modification could also lead to increasingly complex and
uninterpretable internal logic, hindering human understanding, oversight, and control (Sheth et al.,
2025; Anwar et al., 2024; Greenblatt et al., 2024; Ganguli et al., 2022).
Recognizing these challenges, the current implementation and experimental setup of the DGM
incorporates several safeguards. All agent execution and self-modification processes are conducted
within isolated sandboxed environments, limiting their ability to affect the host system, and thereby
mitigating the risk of unintended actions. Each execution within the sandbox is subjected to a strict
time limit, reducing the risk of resource exhaustion or unbounded behavior. The self-improvement
process is currently confined to the well-defined domain of enhancing performance on specific coding
benchmarks by modifying the agent’s own Python codebase, thus limiting the scope of potential
modifications. Additionally, we actively monitor agent performance and code changes, with the
DGM archive providing a traceable lineage of modifications for review. At this stage, we have found
no evidence of harmful or malicious behavior in the generated agents, and the self-modifications have
been primarily focused on improving coding capabilities.
Conversely, a significant potential benefit of the self-improvement paradigm is that it could, in princi-
ple, be directed toward enhancing safety and interpretability themselves. We conduct a preliminary
investigation into how the DGM can be deployed in AI safety settings to develop countermeasures for
FM hallucination (Appendix H). Just as the DGM learns to improve its coding capabilities, it could
potentially discover and integrate better internal safeguards or modify itself for greater transparency
(e.g., incorporating principles akin to Constitutional AI (Bai et al., 2022)), if such properties were
included in its evaluation criteria (Rosser & Foerster, 2025). This suggests a promising, albeit
challenging, pathway in which self-improvement becomes a tool for building more trustworthy AI
systems. Additional research could also explore weaving Constitutional AI in from the start, though
the challenge would be incentivizing the system to retain these directives (an option worth exploring
is to create an unmodifiable part of the system to be able to evaluate at halt the rest).
The DGM demonstrates the potential of self-improving AI while still operating within safe research
boundaries due to the current limitations of frontier FMs and effective mitigations like sandboxing.
We include this safety discussion proactively to raise awareness about the emerging prospect of
self-improving AI systems and their associated safety implications, particularly as these systems
inevitably become more capable (Yudkowsky et al., 2008; Bostrom, 2002; Ecoffet et al., 2020; Bengio
et al., 2024; Clune, 2019). Accordingly, we advocate for continued investigation into the safe and
beneficial evolution of AI-Generating Algorithms (Clune, 2019) and self-improving systems.
6 CONCLUSION AND LIMITATIONS
We introduce the Darwin Gödel Machine (DGM), the first self-improving system powered by FMs
with open-ended exploration, where progress on its evaluation benchmarks can directly translate
into better self-improvement capabilities. We demonstrate the automatic discovery of better tools
and FM systems, resulting in better performance on two benchmarks: SWE-bench and Polyglot.
Through self-improvement and open-ended exploration, the DGM shows a continuous increase in
performance, bringing us one step closer to self-accelerating, self-improving AI systems.
We demonstrate that the DGM can autonomously achieve performance on par with openly available
solutions. However, it still falls short of closed-source SoTA SWE-bench solutions. An open question
is whether running the DGM for longer would continue to yield performance gains and eventually
surpass closed-source solutions. These closed-source solutions often rely on elaborately handcrafted
techniques developed by teams of highly skilled experts. Since FMs have yet to match the capabilities
9
of such experts (e.g., in reasoning), the DGM currently requires extensive compute to discover
improvements. A single run of the DGM on SWE-bench, as presented in Section 4, takes about 2
weeks and incurs significant API costs (Appendix E.1). We hypothesize that further progress will
require more efficient use of computational resources and the development of better reasoning skills.
Since this version of the DGM is mainly powered by FMs, it is inherently limited by the capabilities
of the underlying FM. Hence, an exciting future direction is to extend self-modification beyond just
prompts or FM workflows, to include more computationally intensive methods, such as rewriting
its own training script to update the FM itself. While this version of the DGM focuses on coding,
AI systems are increasingly applied across a wide range of domains (e.g., computer vision, creative
writing). Another promising extension is to develop self-improving AI systems capable of enhancing
themselves beyond just the coding domain. A key assumption in this work is that coding benchmarks
are a good reflection of the agent’s ability to self-improve, since the self-modification task requires
the agent to modify its own codebase. However, one could envision an alternative approach that
co-evolves the target task distribution (Faldor et al., 2025; Wang et al., 2023c), thereby removing
the constraint of self-improvement being tied to a single objective, as in true open-ended processes.
Appendix I presents additional potential directions for future work. As we continue to explore this
powerful technology, we must also keep safety front and center, as discussed in Section 5.
In conclusion, the DGM represents a significant step toward the automation of AI development
through self-improving systems capable of editing their own codebase. While current limitations in
compute and reasoning constrain its full potential, continued advances in FMs and infrastructure may
unlock more powerful and general-purpose self-improvements. Provided that the safety concerns
are carefully navigated (Section 5), the future of self-improving AI systems and AI-Generating
Algorithms (Clune, 2019) holds immense promise to open-endedly evolve AI, continually rewriting
or retraining itself in pursuit of greater capabilities aligned with human values.
ACKNOWLEDGMENTS
This research was supported by the Vector Institute, the Canada CIFAR AI Chairs program, a
grant from Schmidt Futures, an NSERC Discovery Grant, and a generous donation from Rafael
Cosman. Resources used in preparing this research were provided, in part, by the Province of
Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute
(https://vectorinstitute.ai/partnerships/current-partners/). Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reflect the
views of the sponsors. We also thank Aaron Dharna, Ben Norman, Cédric Colas, Sam Devlin, and
Shyam Sudhakaran for insightful discussions and feedback.
